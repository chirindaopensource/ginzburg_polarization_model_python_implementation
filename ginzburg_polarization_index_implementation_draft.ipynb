{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMKMEnaTTwez"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# README.md\n",
        "\n",
        "# A Flexible Measure of Voter Polarization: A Python Implementation\n",
        "\n",
        "<!-- PROJECT SHIELDS -->\n",
        "[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n",
        "[![Python Version](https://img.shields.io/badge/python-3.9%2B-blue.svg)](https://www.python.org/downloads/)\n",
        "[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n",
        "[![Imports: isort](https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat&labelColor=ef8336)](https://pycqa.github.io/isort/)\n",
        "[![Type Checking: mypy](https://img.shields.io/badge/type_checking-mypy-blue)](http://mypy-lang.org/)\n",
        "[![Pandas](https://img.shields.io/badge/pandas-%23150458.svg?style=flat&logo=pandas&logoColor=white)](https://pandas.pydata.org/)\n",
        "[![NumPy](https://img.shields.io/badge/numpy-%23013243.svg?style=flat&logo=numpy&logoColor=white)](https://numpy.org/)\n",
        "[![SciPy](https://img.shields.io/badge/SciPy-%230C55A5.svg?style=flat&logo=scipy&logoColor=white)](https://scipy.org/)\n",
        "[![Matplotlib](https://img.shields.io/badge/Matplotlib-%23ffffff.svg?style=flat&logo=Matplotlib&logoColor=black)](https://matplotlib.org/)\n",
        "[![Seaborn](https://img.shields.io/badge/seaborn-%233776AB.svg?style=flat&logo=python&logoColor=white)](https://seaborn.pydata.org/)\n",
        "[![Jupyter](https://img.shields.io/badge/Jupyter-%23F37626.svg?style=flat&logo=Jupyter&logoColor=white)](https://jupyter.org/)\n",
        "[![arXiv](https://img.shields.io/badge/arXiv-2507.07770-b31b1b.svg)](https://arxiv.org/abs/2507.07770)\n",
        "[![DOI](https://img.shields.io/badge/DOI-10.48550/arXiv.2507.07770-blue)](https://doi.org/10.48550/arXiv.2507.07770)\n",
        "[![Research](https://img.shields.io/badge/Research-Computational%20Social%20Science-green)](https://github.com/chirindaopensource/ginzburg_polarization_model_python_implementation)\n",
        "[![Discipline](https://img.shields.io/badge/Discipline-Political%20Economy-blue)](https://github.com/chirindaopensource/ginzburg_polarization_model_python_implementation)\n",
        "[![Methodology](https://img.shields.io/badge/Methodology-Distributional%20Analysis-orange)](https://github.com/chirindaopensource/ginzburg_polarization_model_python_implementation)\n",
        "[![Data Source](https://img.shields.io/badge/Data%20Source-ANES-lightgrey)](https://electionstudies.org/)\n",
        "[![Year](https://img.shields.io/badge/Year-2025-purple)](https://github.com/chirindaopensource/ginzburg_polarization_model_python_implementation)\n",
        "\n",
        "**Repository:** https://github.com/chirindaopensource/ginzburg_polarization_model_python_implementation\n",
        "\n",
        "**Owner:** 2025 Craig Chirinda (Open Source Projects)\n",
        "\n",
        "This repository contains an **independent** implementation of the research methodology from the 2025 paper entitled **\"A Flexible Measure of Voter Polarization\"** by:\n",
        "\n",
        "*   Boris Ginzburg\n",
        "\n",
        "The project provides a robust, end-to-end Python pipeline for computing and analyzing the flexible polarization index, `P(F, x*)`. This measure moves beyond traditional, mean-centric metrics like variance to provide a high-resolution diagnostic tool. It allows an analyst to measure polarization around any specified point in the ideological spectrum, enabling the precise identification and tracking of specific fault lines within an electorate.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "- [Introduction](#introduction)\n",
        "- [Theoretical Background](#theoretical-background)\n",
        "- [Features](#features)\n",
        "- [Methodology Implemented](#methodology-implemented)\n",
        "- [Core Components (Notebook Structure)](#core-components-notebook-structure)\n",
        "- [Key Callable: execute_full_research_project](#key-callable-execute_full_research_project)\n",
        "- [Prerequisites](#prerequisites)\n",
        "- [Installation](#installation)\n",
        "- [Input Data Structure](#input-data-structure)\n",
        "- [Usage](#usage)\n",
        "- [Output Structure](#output-structure)\n",
        "- [Project Structure](#project-structure)\n",
        "- [Customization](#customization)\n",
        "- [Contributing](#contributing)\n",
        "- [License](#license)\n",
        "- [Citation](#citation)\n",
        "- [Acknowledgments](#acknowledgments)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This project provides a Python implementation of the methodologies presented in the 2025 paper \"A Flexible Measure of Voter Polarization.\" The core of this repository is the iPython Notebook `ginzburg_polarization_index_implementation_draft.ipynb`, which contains a comprehensive suite of functions to compute the `P(F, x*)` index, analyze its dynamics, and compare it against traditional measures.\n",
        "\n",
        "Traditional measures of polarization, such as variance, describe the dispersion of an electorate around a single point—the mean. This can mask crucial asymmetric dynamics. For instance, the center-right may be polarizing while the center-left is coalescing, an effect that a single variance number might miss. The Ginzburg index solves this by making the reference point `x*` a flexible parameter, effectively allowing an analyst to \"scan\" the entire ideological spectrum for polarization.\n",
        "\n",
        "This codebase enables researchers, political analysts, and data scientists to:\n",
        "-   Rigorously compute the `P(F, x*)` index for any ideological distribution.\n",
        "-   Analyze the evolution of polarization at different points on the political spectrum over time.\n",
        "-   Systematically identify the ideological \"cleavage points\" where polarization has increased the most.\n",
        "-   Compare the insights from this flexible measure against traditional metrics.\n",
        "-   Replicate and extend the findings of the original research paper.\n",
        "\n",
        "## Theoretical Background\n",
        "\n",
        "The implemented methods are grounded in distributional analysis and integral calculus, applied to survey data.\n",
        "\n",
        "**A Flexible Definition of Polarization:** The framework begins by defining what it means for one distribution `F̂` to be more polarized than another `F` around a specific point `x*`. This occurs if the probability mass in *any* interval containing `x*` is smaller under `F̂`. This leads to a practical single-crossing condition on the Cumulative Distribution Functions (CDFs).\n",
        "\n",
        "**The Polarization Index `P(F, x*)`:** To provide a scalar measure, the paper defines the polarization index. The formula is designed to increase as probability mass shifts away from the central point `x*` towards both tails of the distribution.\n",
        "$P(F, x^*) := \\frac{\\int_{\\min\\{X\\}}^{x^*} F(x) dx}{x^* - \\min\\{X\\}} - \\frac{\\int_{x^*}^{\\max\\{X\\}} F(x) dx}{\\max\\{X\\} - x^*} + 1$\n",
        "For discrete survey data, the integrals are calculated exactly as the sum of areas of rectangles under the empirical CDF's step function.\n",
        "\n",
        "**Cleavage Point Finder:** The framework can be inverted. Instead of specifying `x*` and measuring polarization, the pipeline can perform a grid search across all possible `x*` values to find the one where the percentage increase in `P(F, x*)` between two time periods is maximized. This identifies the most significant emerging ideological fault line.\n",
        "\n",
        "## Features\n",
        "\n",
        "The provided iPython Notebook (`ginzburg_polarization_index_implementation_draft.ipynb`) implements the full research pipeline, including:\n",
        "\n",
        "-   **Parameter Validation:** Rigorous checks for all input data and configurations to ensure methodological compliance.\n",
        "-   **Data Cleansing:** Robust handling of survey-specific missing value codes.\n",
        "-   **Weighted Statistics:** Correct application of survey weights for all calculations, including CDF construction and traditional measures.\n",
        "-   **Exact `P(F, x*)` Calculation:** A numerically stable and mathematically precise implementation of the polarization index for discrete data.\n",
        "-   **Automated Analysis Suite:** Functions to systematically analyze temporal trends, election-year effects, and comparative performance against traditional metrics.\n",
        "-   **Cleavage Point Finder:** An algorithm to scan the ideological spectrum and identify points of maximum polarization increase.\n",
        "-   **Theoretical Extensions:** Computational models for affective polarization and the effects of issue salience.\n",
        "-   **Robustness Checks:** A framework for testing the sensitivity of results to key methodological choices (e.g., weighted vs. unweighted analysis).\n",
        "-   **Publication-Quality Visualization:** A suite of functions to generate the key figures from the paper.\n",
        "\n",
        "## Methodology Implemented\n",
        "\n",
        "The core analytical steps directly implement the methodology from the paper:\n",
        "\n",
        "1.  **Data Preparation (Tasks 1-2):** The pipeline ingests raw ANES data, cleans it by handling missing value codes, and constructs a weighted empirical Cumulative Distribution Function (CDF) for each survey year and ideological scale.\n",
        "2.  **Polarization Calculation (Tasks 3-4):** It computes the `P(F, x*)` index for every specified `(year, scale, x*)` combination using the formula from Definition 2.\n",
        "3.  **Temporal and Event Analysis (Tasks 5-6):** The pipeline analyzes the evolution of `P(F, x*)` over time and its short-term changes around elections, replicating the analysis in Figures 1, 2, and 3 of the paper.\n",
        "4.  **Cleavage Point Identification (Tasks 7-8):** It implements the grid search algorithm to find the `x*` that maximizes the percentage increase in polarization between two periods.\n",
        "5.  **Comparative Analysis (Tasks 9-10):** The pipeline computes traditional measures (e.g., variance) and systematically compares their trends to those of the `P(F, x*)` index at various points, quantifying where the measures converge and diverge.\n",
        "\n",
        "## Core Components (Notebook Structure)\n",
        "\n",
        "The `ginzburg_polarization_index_implementation_draft.ipynb` notebook is structured as a logical pipeline with modular functions for each task:\n",
        "\n",
        "-   **Task 0: `validate_parameters`**: The initial quality gate for all inputs.\n",
        "-   **Task 1: `clean_anes_data`**: Handles data quality and missing codes.\n",
        "-   **Task 2: `preprocess_for_polarization`**: The core CDF generation engine.\n",
        "-   **Task 3-4: `calculate_polarization_index`, `compute_all_polarization_measures`**: The main polarization index calculation.\n",
        "-   **Task 5-10**: A suite of analysis functions for temporal, election, cleavage, and comparative analysis.\n",
        "-   **Task 11: `calculate_affective_polarization`, `simulate_issue_salience_effect`**: Implementation of the theoretical models.\n",
        "-   **Task 12-14**: High-level orchestrators (`run_polarization_pipeline`, `run_robustness_analysis`, `execute_full_research_project`) that run the entire workflow.\n",
        "\n",
        "## Key Callable: execute_full_research_project\n",
        "\n",
        "The central function in this project is `execute_full_research_project`. It orchestrates the entire analytical workflow from raw data to final results, including robustness checks and report generation.\n",
        "\n",
        "```python\n",
        "def execute_full_research_project(\n",
        "    anes_df: pd.DataFrame,\n",
        "    params: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes the complete, end-to-end polarization research project.\n",
        "\n",
        "    This master orchestrator function serves as the single entry point to run\n",
        "    the entire analysis suite, from raw data to final report assets. It encapsulates\n",
        "    the full research workflow, including the baseline analysis, robustness checks,\n",
        "    and the generation of all tables and visualizations.\n",
        "\n",
        "    Args:\n",
        "        anes_df (pd.DataFrame): The raw ANES survey data.\n",
        "        params (Dict[str, Any]): A comprehensive dictionary containing all\n",
        "            parameters required for every stage of the analysis.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A master dictionary containing the complete project results.\n",
        "    \"\"\"\n",
        "    # ... (implementation is in the notebook)\n",
        "```\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "-   Python 3.9+\n",
        "-   Core dependencies as listed in `requirements.txt`: `pandas`, `numpy`, `scipy`, `matplotlib`, `seaborn`.\n",
        "\n",
        "## Installation\n",
        "\n",
        "1.  **Clone the repository:**\n",
        "    ```sh\n",
        "    git clone https://github.com/chirindaopensource/ginzburg_polarization_model_python_implementation.git\n",
        "    cd ginzburg_polarization_model_python_implementation\n",
        "    ```\n",
        "\n",
        "2.  **Create and activate a virtual environment (recommended):**\n",
        "    ```sh\n",
        "    python -m venv venv\n",
        "    source venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\n",
        "    ```\n",
        "\n",
        "3.  **Install Python dependencies from `requirements.txt`:**\n",
        "    ```sh\n",
        "    pip install -r requirements.txt\n",
        "    ```\n",
        "\n",
        "## Input Data Structure\n",
        "\n",
        "The primary input is a `pandas.DataFrame` with the following required columns:\n",
        "-   `respondent_id`: A unique identifier for each respondent.\n",
        "-   `year`: A string/object representing the survey wave (e.g., `'2004a'`, `'2016b'`).\n",
        "-   `left_right`: The respondent's self-placement on the 0-10 left-right scale.\n",
        "-   `liberal_conservative`: The respondent's self-placement on the 1-7 liberal-conservative scale.\n",
        "-   `weight`: The full-sample survey weight for that respondent.\n",
        "\n",
        "## Usage\n",
        "\n",
        "### **User Guide: Deploying the End-to-End Polarization Analysis Pipeline**\n",
        "\n",
        "This section provides a practical, step-by-step guide to utilizing the `execute_full_research_project` function. This function is the single entry point to the entire analytical library, designed for robust, reproducible, and comprehensive analysis of voter polarization.\n",
        "\n",
        "#### **Step 1: Data Acquisition and Preparation**\n",
        "\n",
        "The pipeline requires a single, consolidated `pandas.DataFrame` as its primary data input. This DataFrame must be prepared *before* calling the main function and must adhere to a strict schema.\n",
        "\n",
        "*   **Action:** The analyst must first acquire the necessary survey data (e.g., from the ANES Data Center). The data from multiple years and waves should be merged into one file.\n",
        "*   **Schema:** The resulting DataFrame must contain the following columns with these exact names:\n",
        "    *   `respondent_id`: A unique identifier for each respondent.\n",
        "    *   `year`: A string or object representing the survey wave (e.g., `'2004a'`, `'2016b'`).\n",
        "    *   `left_right`: The respondent's self-placement on the 0-10 left-right scale.\n",
        "    *   `liberal_conservative`: The respondent's self-placement on the 1-7 liberal-conservative scale.\n",
        "    *   `weight`: The full-sample survey weight provided by ANES for that specific survey wave.\n",
        "\n",
        "**Code Snippet: Creating a Mock DataFrame**\n",
        "\n",
        "For demonstration purposes, we will construct a small, mock `DataFrame` that conforms to this schema. In a real-world scenario, this `anes_df` would be the result of loading and merging actual ANES `.dta` or `.csv` files.\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# In a real application, this DataFrame would be loaded from a file.\n",
        "# df = pd.read_csv(\"path/to/your/merged_anes_data.csv\")\n",
        "# For this example, we create a mock DataFrame.\n",
        "\n",
        "data = {\n",
        "    'respondent_id': [f'R_{i}' for i in range(100)],\n",
        "    'year': np.random.choice(['2012', '2016a', '2016b', '2020'], 100),\n",
        "    'left_right': np.random.choice(list(range(11)) + [98], 100), # Include a missing code\n",
        "    'liberal_conservative': np.random.choice(list(range(1, 8)) + [99], 100), # Include a missing code\n",
        "    'weight': np.random.uniform(0.5, 2.5, 100)\n",
        "}\n",
        "anes_df = pd.DataFrame(data)\n",
        "\n",
        "print(\"Sample of the prepared input DataFrame:\")\n",
        "print(anes_df.head())\n",
        "```\n",
        "\n",
        "#### **Step 2: Constructing the Master Parameters Dictionary**\n",
        "\n",
        "The `execute_full_research_project` function is controlled by a single, comprehensive parameters dictionary. This object encapsulates every tunable setting for the entire analysis, ensuring perfect reproducibility. We will construct this dictionary section by section, using the default values specified in the original project brief.\n",
        "\n",
        "**Code Snippet: Defining the `params` Dictionary**\n",
        "\n",
        "```python\n",
        "# Initialize the master parameters dictionary.\n",
        "params = {}\n",
        "\n",
        "# --- Central Points of Interest (x*) ---\n",
        "# These are the points around which polarization will be measured for each scale.\n",
        "params['central_points_params'] = {\n",
        "    \"left_right_x_stars\": [1, 2, 3, 4, 5, 6, 7, 8, 9], # Note: Excludes boundaries 0 and 10\n",
        "    \"liberal_conservative_x_stars\": [2, 3, 4, 5, 6] # Note: Excludes boundaries 1 and 7\n",
        "}\n",
        "\n",
        "# --- Policy Space Boundaries ---\n",
        "# These define the theoretical min and max of each ideological scale.\n",
        "params['boundaries_params'] = {\n",
        "    \"left_right_boundaries\": {'min': 0, 'max': 10},\n",
        "    \"liberal_conservative_boundaries\": {'min': 1, 'max': 7}\n",
        "}\n",
        "\n",
        "# --- Integration Parameters ---\n",
        "# Defines the numerical method for the P(F,x*) calculation.\n",
        "# Note: The current implementation is optimized for 'trapezoidal' on discrete data.\n",
        "params['integration_params'] = {\n",
        "    'method': 'trapezoidal',\n",
        "    'num_points': 1000\n",
        "}\n",
        "\n",
        "# --- Cleavage Finder Parameters ---\n",
        "# Defines the time periods to scan for the maximum increase in polarization.\n",
        "params['cleavage_finder_params'] = {\n",
        "    'time_points': [('2016a', '2020')], # Using years available in our mock data\n",
        "    'potential_x_stars': params['central_points_params']\n",
        "}\n",
        "\n",
        "# --- Analysis-Specific Parameters ---\n",
        "# Defines midpoints for partitioning temporal plots and centrist definitions.\n",
        "params['midpoints'] = {\n",
        "    'left_right': 5,\n",
        "    'liberal_conservative': 4\n",
        "}\n",
        "params['centrist_definitions'] = {\n",
        "    'left_right': [4, 5, 6],\n",
        "    'liberal_conservative': [3, 4, 5]\n",
        "}\n",
        "params['election_years_for_analysis'] = [2016] # Using year available in mock data\n",
        "\n",
        "# --- Theoretical Extension Parameters ---\n",
        "# Defines parameters for the issue salience simulation.\n",
        "params['salience_simulation_params'] = {\n",
        "    'salience_alphas': [0.1, 0.3, 0.5, 0.7, 0.9],\n",
        "    'common_value_dist': {'low': 4.8, 'high': 5.2},\n",
        "    'divisive_issue_dist': {'low': 0, 'high': 10},\n",
        "    'polarization_params': {\n",
        "        'x_star': 5, # The x* to calculate polarization around in the simulation\n",
        "        'boundaries': params['boundaries_params']['left_right_boundaries']\n",
        "    },\n",
        "    'random_seed': 42 # For reproducibility\n",
        "}\n",
        "\n",
        "# --- Optional: ANES Missing Value Codes ---\n",
        "# This can be customized if different codes are used in the data.\n",
        "params['missing_value_map'] = {\n",
        "    'left_right': [98, 99],\n",
        "    'liberal_conservative': [98, 99]\n",
        "}\n",
        "\n",
        "print(\"\\nMaster parameters dictionary constructed successfully.\")\n",
        "```\n",
        "\n",
        "#### **Step 3: Executing the Pipeline and Inspecting Results**\n",
        "\n",
        "With the input `DataFrame` and the `params` dictionary prepared, the entire research project can be executed with a single function call. The function will print its progress through the various stages of the analysis.\n",
        "\n",
        "**Code Snippet: Running the Master Orchestrator**\n",
        "\n",
        "```python\n",
        "# First, ensure the full library of functions is loaded in your environment.\n",
        "# from ginzburg_polarization_model_python_implementation import execute_full_research_project\n",
        "\n",
        "# Execute the entire research project.\n",
        "# This single call runs validation, cleaning, all calculations, analyses,\n",
        "# robustness checks, and reporting.\n",
        "project_results = execute_full_research_project(\n",
        "    anes_df=anes_df,\n",
        "    params=params\n",
        ")\n",
        "```\n",
        "\n",
        "The returned `project_results` object is a deeply nested dictionary containing every artifact generated during the run. An analyst can now programmatically access any piece of the analysis for inspection, custom plotting, or further work.\n",
        "\n",
        "**Code Snippet: Accessing and Inspecting Key Outputs**\n",
        "\n",
        "```python\n",
        "# --- Inspecting the Main Analysis Results ---\n",
        "print(\"\\n--- Example: Inspecting Key Outputs from the Main Analysis ---\")\n",
        "\n",
        "# Access the main results dictionary\n",
        "main_analysis_results = project_results['main_analysis']['results']\n",
        "\n",
        "# 1. View the summary of the cleavage point analysis\n",
        "print(\"\\nCleavage Point Analysis Summary:\")\n",
        "cleavage_summary = main_analysis_results['cleavage_analysis']\n",
        "print(cleavage_summary)\n",
        "\n",
        "# 2. View the summary of the comparative analysis (correlation)\n",
        "print(\"\\nCorrelation Summary (Traditional vs. Flexible Measures):\")\n",
        "correlation_summary = main_analysis_results['comparative_framework']['correlation_summary']\n",
        "print(correlation_summary)\n",
        "\n",
        "# 3. Access a specific plot (e.g., temporal trends for the left-right scale)\n",
        "report_assets = project_results['main_analysis']['report_assets']\n",
        "temporal_plot_lr = report_assets['plots']['temporal_trends_left_right']\n",
        "# To display the plot in a Jupyter environment:\n",
        "# temporal_plot_lr.show()\n",
        "# To save the plot to a file:\n",
        "# temporal_plot_lr.savefig(\"temporal_trends_left_right.png\", dpi=300)\n",
        "print(\"\\nTemporal trend plot for 'left_right' scale has been generated.\")\n",
        "\n",
        "# --- Inspecting the Robustness Analysis Results ---\n",
        "print(\"\\n--- Example: Comparing Robustness Check Results ---\")\n",
        "\n",
        "# Compare the cleavage point found in the weighted vs. unweighted scenarios\n",
        "try:\n",
        "    weighted_cleavage = project_results['robustness_analysis']['weighted']['cleavage_analysis']\n",
        "    unweighted_cleavage = project_results['robustness_analysis']['unweighted']['cleavage_analysis']\n",
        "\n",
        "    print(\"\\nCleavage points from 'weighted' analysis:\")\n",
        "    print(weighted_cleavage[['cleavage_point', 'relative_position']])\n",
        "\n",
        "    print(\"\\nCleavage points from 'unweighted' analysis:\")\n",
        "    print(unweighted_cleavage[['cleavage_point', 'relative_position']])\n",
        "except KeyError:\n",
        "    print(\"\\nRobustness check for one or more scenarios may have failed.\")\n",
        "\n",
        "```\n",
        "This example provides a complete, end-to-end workflow, demonstrating how a user can prepare their data, configure the analysis, execute the entire pipeline with one command, and access the structured, high-value outputs for interpretation.\n",
        "\n",
        "## Output Structure\n",
        "\n",
        "The `execute_full_research_project` function returns a single, comprehensive dictionary with the following top-level keys:\n",
        "\n",
        "-   `main_analysis`: Contains the results of the primary pipeline run.\n",
        "    -   `results`: A dictionary of all key data artifacts and analytical DataFrames (e.g., `polarization_results`, `cleavage_analysis`).\n",
        "    -   `report_assets`: A dictionary containing the generated `matplotlib` figures and formatted LaTeX/HTML tables.\n",
        "-   `robustness_analysis`: Contains the results from the different robustness scenarios (e.g., 'weighted' vs. 'unweighted'), allowing for direct comparison.\n",
        "\n",
        "## Project Structure\n",
        "\n",
        "```\n",
        "ginzburg_polarization_model_python_implementation/\n",
        "│\n",
        "├── ginzburg_polarization_index_implementation_draft.ipynb  # Main implementation notebook\n",
        "├── requirements.txt                                      # Python package dependencies\n",
        "├── LICENSE                                                 # MIT license file\n",
        "└── README.md                                               # This documentation file\n",
        "```\n",
        "\n",
        "## Customization\n",
        "\n",
        "The pipeline is highly customizable via the master `params` dictionary. Users can easily modify:\n",
        "-   The lists of `x_stars` to analyze for each scale.\n",
        "-   The `time_points` for the cleavage finder.\n",
        "-   The `midpoints` and `centrist_definitions` for analysis and comparison.\n",
        "-   The parameters for the theoretical simulations.\n",
        "\n",
        "## Contributing\n",
        "\n",
        "Contributions are welcome. Please fork the repository, create a feature branch, and submit a pull request with a clear description of your changes. Adherence to PEP 8, type hinting, and comprehensive docstrings is required.\n",
        "\n",
        "## License\n",
        "\n",
        "This project is licensed under the MIT License. See the `LICENSE` file for details.\n",
        "\n",
        "## Citation\n",
        "\n",
        "If you use this code or the methodology in your research, please cite the original paper:\n",
        "\n",
        "```bibtex\n",
        "@article{ginzburg2025flexible,\n",
        "  title={A Flexible Measure of Voter Polarization},\n",
        "  author={Ginzburg, Boris},\n",
        "  journal={arXiv preprint arXiv:2507.07770},\n",
        "  year={2025}\n",
        "}\n",
        "```\n",
        "\n",
        "For the implementation itself, you may cite this repository:\n",
        "```\n",
        "Chirinda, C. (2025). A Python Implementation of the Ginzburg Flexible Polarization Model.\n",
        "GitHub repository: https://github.com/chirindaopensource/ginzburg_polarization_model_python_implementation\n",
        "```\n",
        "\n",
        "## Acknowledgments\n",
        "\n",
        "-   Credit to Boris Ginzburg for the novel theoretical framework and the flexible polarization measure.\n",
        "-   Thanks to the developers of the `pandas`, `numpy`, `scipy`, `matplotlib`, and `seaborn` libraries, which are the foundational pillars of this analytical pipeline.\n",
        "\n",
        "--\n",
        "\n",
        "*This README was generated based on the structure and content of `ginzburg_polarization_index_implementation_draft.ipynb` and follows best practices for research software documentation.*"
      ],
      "metadata": {
        "id": "utWzciMkvkUS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paper\n",
        "\n",
        "Title: \"A Flexible Measure of Voter Polarization\"\n",
        "\n",
        "Link: https://arxiv.org/abs/2507.07770\n",
        "\n",
        "E-Journal Submission Date: 11th of July 2025\n",
        "\n",
        "Author: Boris Ginzburg\n",
        "\n",
        "Abstract:\n",
        "\n",
        "This paper introduces a definition of ideological polarization of an electorate around a particular central point. By being flexible about the location or width of the center, this measure enables the researcher to analyze polarization around any point of interest. The paper then applies this approach to US voter survey data between 2004 and 2020, showing how polarization between right-of-center voters and the rest of the electorate was increasing gradually, while polarization between left-wingers and the rest was originally constant and then rose steeply. It also shows how, following elections, polarization around left-wing positions decreased while polarization around right-wing positions increased. Furthermore, the paper shows how this measure can be used to find cleavage points around which polarization changed the most. I then show how ideological polarization as defined here is related to other phenomena, such as affective polarization and increased salience of divisive issues.\n",
        "\n"
      ],
      "metadata": {
        "id": "G6isTgdmUSIn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "### **Summary of \"A Flexible Measure of Voter Polarization\"**\n",
        "\n",
        "This paper introduces a novel theoretical framework and an associated numerical measure for ideological polarization. Its primary innovation is its flexibility, allowing a researcher to analyze polarization around *any* chosen central point, rather than being restricted to traditional measures centered on the mean or median.\n",
        "\n",
        "#### **Step 1: The Core Problem and Motivation**\n",
        "\n",
        "The paper begins by identifying a key limitation in the existing literature on mass polarization. While polarization is often described as the \"disappearance of the center,\" common statistical measures used to capture this phenomenon are often inadequate or too restrictive:\n",
        "\n",
        "*   **Measures like variance or kurtosis** are, by construction, measures of dispersion around the distribution's mean. This implicitly assumes the mean is the only relevant \"center,\" which may not be true in many political or theoretical contexts (e.g., the median voter, a neutral policy point).\n",
        "*   **Measures based on the share of \"centrists\"** require the researcher to arbitrarily define the boundaries of the center (e.g., scores of 4-6 on a 10-point scale). The results can be highly sensitive to this choice.\n",
        "*   **Group-based measures** (e.g., distance between Republican and Democrat means) require pre-defined group identities and are less useful for analyzing polarization within a heterogeneous electorate as a whole.\n",
        "\n",
        "The author proposes a measure that overcomes these limitations by being agnostic about the location of the center, allowing the researcher to define it based on the specific research question.\n",
        "\n",
        "#### **Step 2: A Formal Definition of Polarization (The Partial Order)**\n",
        "\n",
        "The theoretical core of the paper is a new definition of polarization based on a partial ordering of distributions.\n",
        "\n",
        "*   **Definition 1:** A distribution `F̂` is said to dominate another distribution `F` in polarization around a specific point `x*` if, for *any* interval that contains `x*`, the share of the population within that interval is smaller under `F̂` than under `F`.\n",
        "*   **Intuition:** This formalizes the idea of the \"hollowing out\" of the center. If polarization increases, fewer people should be found near the central point `x*`, regardless of how narrowly or broadly one defines \"near.\"\n",
        "*   **Proposition 1 (The Single-Crossing Condition):** The paper provides a simple and powerful necessary and sufficient condition for this dominance relationship. `F̂` is more polarized around `x*` than `F` if and only if the cumulative distribution function (CDF) of `F̂` is everywhere above the CDF of `F` to the left of `x*`, and everywhere below it to the right of `x*`.\n",
        "    *   `F̂(x) ≥ F(x)` for all `x < x*`\n",
        "    *   `F̂(x) ≤ F(x)` for all `x > x*`\n",
        "    This is equivalent to saying that for the population to the left of `x*`, `F` first-order stochastically dominates `F̂` (a shift towards `x*`), while for the population to the right of `x*`, `F̂` first-order stochastically dominates `F` (a shift away from `x*`).\n",
        "\n",
        "#### **Step 3: A Numerical Measure of Polarization (The Index)**\n",
        "\n",
        "The partial ordering from Step 2 cannot compare all pairs of distributions. To address this, the paper introduces a numerical index, `P(F, x*)`, that is consistent with the partial order but allows for the comparison of any two distributions.\n",
        "\n",
        "*   **Definition 2:** The index `P(F, x*)` is defined based on the normalized integrals of the CDF on either side of the central point `x*`.\n",
        "*   **Properties (Proposition 2):** This index is well-behaved.\n",
        "    1.  It is normalized to lie within the `[0, 1]` interval, making it comparable across different central points `x*`.\n",
        "    2.  Crucially, it is consistent with the partial order: if `F̂` dominates `F` in polarization around `x*`, then `P(F̂, x*) > P(F, x*)`.\n",
        "\n",
        "#### **Step 4: Empirical Application to the US Electorate**\n",
        "\n",
        "The paper demonstrates the utility of this new framework by applying it to American National Election Studies (ANES) data from 2004 to 2020, analyzing both left-right and liberal-conservative self-placement scales. The findings are more nuanced than those from traditional measures like variance.\n",
        "\n",
        "*   **Asymmetric Polarization Trends:** While variance shows a general increase in polarization after 2012, the `P(F, x*)` measure reveals a more complex dynamic:\n",
        "    *   **Right-of-Center:** Polarization around various right-wing and conservative positions showed a *gradual and steady increase* across the entire 2004-2020 period.\n",
        "    *   **Left-of-Center:** Polarization around various left-wing and liberal positions was relatively stable or even decreased until 2012, after which it began to *increase sharply*.\n",
        "*   **Analyzing Short-Term Events:** The measure can detect subtle shifts during election cycles. Following the 2004 and 2016 elections, polarization around liberal positions *decreased*, while polarization around conservative positions *increased*. A simple variance measure would miss this asymmetric shift.\n",
        "*   **Finding Cleavages:** The framework can be used in reverse: instead of specifying a center `x*`, one can search for the `x*` around which polarization has changed the most. The analysis shows that in the post-2012 period, the largest percentage increases in polarization occurred around points slightly to the *left of the center* of the ideological scale, identifying this as a key cleavage.\n",
        "\n",
        "#### **Step 5: Linking Ideological Polarization to Other Phenomena**\n",
        "\n",
        "The paper concludes by formally connecting its definition of polarization to two other major concepts in political science.\n",
        "\n",
        "*   **Affective Polarization (Proposition 3):** The paper presents a model where voters' dislike of the \"other side\" (affective polarization) is a function of the ideological distance to that group. It proves that an increase in ideological polarization (as defined by the paper's partial order) necessarily leads to an increase in the average level of affective polarization. This provides a formal microfoundation for the empirically observed link between the two.\n",
        "*   **Salience of Divisive Issues (Proposition 4):** A second model conceptualizes a voter's overall ideology as a weighted average of their positions on a consensual (\"common-value\") issue and a divisive issue. The paper shows that increasing the salience (weight) of the more divisive issue causes an increase in ideological polarization as defined in the paper.\n",
        "\n",
        "### **Overall Contribution**\n",
        "\n",
        "The paper's contribution is both methodological and substantive. It provides a rigorous yet intuitive framework that:\n",
        "1.  **Generalizes** the concept of polarization as the \"disappearance of the center\" to any point of interest.\n",
        "2.  **Enables** a more granular and nuanced empirical analysis, revealing asymmetric dynamics missed by traditional aggregate measures.\n",
        "3.  **Offers** a tool to endogenously identify the ideological \"cleavage points\" where political division is growing fastest.\n",
        "4.  **Builds theoretical bridges** by formally linking this measure of ideological polarization to the distinct concepts of affective polarization and issue salience."
      ],
      "metadata": {
        "id": "_FmKyMikVyWk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Essential Modules"
      ],
      "metadata": {
        "id": "pcwU-JBoCGzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "#\n",
        "#  A Flexible Measure of Voter Polarization: A Computational Implementation\n",
        "#\n",
        "#  This module provides a complete, production-grade implementation of the\n",
        "#  analytical framework presented in \"A Flexible Measure of Voter Polarization\"\n",
        "#  by Boris Ginzburg. It includes functions for data validation, cleansing,\n",
        "#  preprocessing, core polarization calculation, and a suite of advanced\n",
        "#  analytical and reporting tools.\n",
        "#\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Consolidated Imports ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "from typing import Dict, Any, List, Tuple, Callable, Optional, Set, Union\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import percentileofscore\n",
        "\n"
      ],
      "metadata": {
        "id": "lTHllKX3CKv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation"
      ],
      "metadata": {
        "id": "9EZlFrpXCNIw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Draft 1\n",
        "\n",
        "### Inputs, Processes and Outputs (IPO) Analysis of Key Callables\n",
        "\n",
        "### **I-P-O Analysis of the Function Library**\n",
        "\n",
        "#### **Task 0: Parameter Validation**\n",
        "\n",
        "*   **Callable:** `validate_parameters` (and its helpers: `_validate_dataframe`, `_validate_boundaries`, `_validate_central_points`, `_validate_integration_params`, `_validate_cleavage_finder_params`)\n",
        "    *   **Inputs:** Raw `anes_df` DataFrame and all parameter dictionaries (`central_points_params`, `boundaries_params`, etc.).\n",
        "    *   **Processes:** This function performs no data transformation. It is a pure validation gatekeeper. It systematically inspects the structure, types, and values of all inputs against a set of predefined rules derived from the logical and mathematical constraints of the subsequent analysis. It checks for required columns, valid years, logical boundary conditions (`min < max`), and cross-validates parameters (e.g., ensuring years specified for cleavage analysis exist in the data).\n",
        "    *   **Outputs:** None. The function's output is binary: it either completes silently, signifying that all inputs are valid, or it raises a specific, informative `ValueError` or `TypeError`, halting the pipeline before any computation occurs.\n",
        "    *   **Role in Research Pipeline:** This callable serves as the foundational \"pre-flight check\" for the entire research project. It ensures that all data and parameters are sound, preventing errors in downstream calculations and guaranteeing the reproducibility and integrity of the analysis from the outset. It implements the implicit requirement for valid inputs that underpins every subsequent calculation.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Task 1: Data Cleansing**\n",
        "\n",
        "*   **Callable:** `clean_anes_data`\n",
        "    *   **Inputs:** The raw `anes_df` DataFrame.\n",
        "    *   **Processes:** The function executes a multi-step data purification process.\n",
        "        1.  It scans all numeric columns for non-finite values (`np.inf`, `-np.inf`) and transforms them into a standard missing value representation (`np.nan`).\n",
        "        2.  It takes survey-specific missing value codes (e.g., 98, 99) in the ideological columns and transforms them also into `np.nan`.\n",
        "        3.  It then applies a listwise deletion, removing any row (respondent) that has a `np.nan` value in any of the critical analytical columns (`left_right`, `liberal_conservative`, `weight`).\n",
        "        4.  Finally, it resets the DataFrame's index to be a clean, contiguous integer sequence.\n",
        "    *   **Outputs:** A new, cleaned `pandas.DataFrame` where the core analytical columns are guaranteed to be free of missing or invalid data.\n",
        "    *   **Role in Research Pipeline:** This function prepares the raw survey data for rigorous quantitative analysis. It operationalizes the standard practice of handling non-substantive responses (\"Don't know,\" \"Refused\") and ensures the dataset used for modeling is complete with respect to the variables of interest. This is a prerequisite for constructing valid probability distributions.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Task 2: Data Preprocessing**\n",
        "\n",
        "*   **Callable:** `preprocess_for_polarization`\n",
        "    *   **Inputs:** The `cleaned_df` DataFrame from Task 1.\n",
        "    *   **Processes:** This function transforms the flat, clean data into a hierarchical, computationally efficient structure.\n",
        "        1.  For each survey `year`, it normalizes the `weight` column so that all weights within that year sum to 1.0, creating probabilistic weights.\n",
        "        2.  It then iterates through each ideological `scale`. For each `(year, scale)` pair, it sorts the respondents by their ideological position.\n",
        "        3.  It constructs the empirical Probability Mass Function (PMF) by grouping by unique positions and summing the normalized weights.\n",
        "        4.  It then constructs the empirical Cumulative Distribution Function (CDF), `F(x)`, by calculating the cumulative sum of the PMF.\n",
        "    *   **Outputs:** A nested dictionary of the form `{year: {scale: {'positions': np.ndarray, 'weights': np.ndarray, 'cdf': pd.DataFrame}}}`. This structure contains the empirical distribution `F` for every year and scale, ready for analysis.\n",
        "    *   **Role in Research Pipeline:** This callable is the direct computational counterpart to the concept of an electorate's ideological distribution, denoted by `F` throughout the paper. It takes the raw survey responses and produces the precise mathematical object, the empirical CDF, upon which the entire polarization measurement is based.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Task 3 & 4: Polarization Calculation**\n",
        "\n",
        "*   **Callables:** `calculate_polarization_index` and `compute_all_polarization_measures`\n",
        "    *   **Inputs:** The `preprocessed_data` dictionary containing the CDFs, and the `central_points_params` and `boundaries_params` dictionaries.\n",
        "    *   **Processes:** The `compute_all_polarization_measures` function orchestrates a systematic grid search. It iterates through every `(year, scale, x*)` combination. In each iteration, it calls `calculate_polarization_index`, which performs the core calculation. This core function implements the exact formula for the polarization index from Definition 2 of the paper. For the discrete empirical CDF, it calculates the integrals as the exact sum of the areas of rectangles under the step-function curve.\n",
        "    *   **Outputs:** A single `pandas.DataFrame` with a `(year, scale, x_star)` MultiIndex, containing the computed `polarization_value` for every point in the parameter space.\n",
        "    *   **Role in Research Pipeline:** This is the central implementation of the paper's primary contribution. It computes the flexible measure of polarization, `P(F, x*)`.\n",
        "    *   **Equation Implemented:**\n",
        "        $P(F, x^*) := \\frac{\\int_{\\min\\{X\\}}^{x^*} F(x) dx}{x^* - \\min\\{X\\}} - \\frac{\\int_{x^*}^{\\max\\{X\\}} F(x) dx}{\\max\\{X\\} - x^*} + 1$\n",
        "\n",
        "--\n",
        "\n",
        "#### **Task 5, 6, 8, 10: Analysis Suite**\n",
        "\n",
        "*   **Callables:** `prepare_temporal_analysis`, `analyze_election_year_effects`, `identify_and_analyze_cleavage_points`, `create_comparative_analysis_framework`\n",
        "    *   **Inputs:** The `polarization_results` DataFrame and, for comparison, the `traditional_measures` DataFrame.\n",
        "    *   **Processes:** This suite of functions transforms and analyzes the computed polarization results.\n",
        "        *   `prepare_temporal_analysis` reshapes the data for time-series plotting, replicating the panel structure of Figures 1 & 2.\n",
        "        *   `analyze_election_year_effects` isolates pre/post election waves and calculates the change `ΔP`, replicating the analysis of Figure 3.\n",
        "        *   `identify_and_analyze_cleavage_points` applies the Cleavage Point Finder algorithm to locate the `x*` that maximizes the percentage increase in polarization over a period, as discussed in the \"Finding cleavages\" section.\n",
        "        *   `create_comparative_analysis_framework` joins the `P(F, x*)` results with traditional measures (variance, etc.) and computes their correlation to identify areas of analytical divergence and convergence.\n",
        "    *   **Outputs:** A collection of analysis-ready DataFrames and summary tables.\n",
        "    *   **Role in Research Pipeline:** These functions execute the various applications of the `P(F, x*)` index demonstrated in the paper. They show how the flexible measure can be used to gain more detailed insights into polarization dynamics over time, around specific events, and in comparison to older methods.\n",
        "    *   **Algorithm Implemented (`find_cleavage_points`):**\n",
        "        1.  For a given period `(t1, t2)` and `scale`:\n",
        "        2.  For each candidate `x^*` in the policy space:\n",
        "        3.  Calculate `increase = 100 \\times (P(F_{t2}, x^*) - P(F_{t1}, x^*)) / P(F_{t1}, x^*)`\n",
        "        4.  Return the `x^*` that maximizes `increase`.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Task 9: Traditional Measures**\n",
        "\n",
        "*   **Callable:** `compute_traditional_measures` (and its helper `_weighted_descriptive_stats`)\n",
        "    *   **Inputs:** The `cleaned_df` DataFrame.\n",
        "    *   **Processes:** This function calculates standard, non-flexible measures of polarization. It computes the weighted variance of the ideological distributions and the weighted proportion of \"centrist\" voters for each year and scale.\n",
        "    *   **Outputs:** A `pandas.DataFrame` indexed by `(year, scale)` containing the values for these traditional metrics.\n",
        "    *   **Role in Research Pipeline:** This callable provides the analytical baseline. As the paper argues, traditional measures like variance can mask underlying dynamics. By computing these measures accurately, we create the necessary benchmark to demonstrate the superior diagnostic power of the `P(F, x*)` index in the comparative analysis (Task 10).\n",
        "\n",
        "--\n",
        "\n",
        "#### **Task 11: Theoretical Extensions**\n",
        "\n",
        "*   **Callables:** `calculate_affective_polarization` and `simulate_issue_salience_effect`\n",
        "    *   **Inputs:** For affective polarization: `positions`, `weights`, a dividing `x*`, and an animosity function `g`. For issue salience: simulation parameters (`alpha` values, distribution definitions).\n",
        "    *   **Processes:**\n",
        "        *   `calculate_affective_polarization` implements the model linking ideological distance to animosity, calculating the total weighted animosity in the electorate.\n",
        "        *   `simulate_issue_salience_effect` implements the model where ideology is a weighted average of two issues. It simulates electorates with varying issue salience (`alpha`) and calculates the resulting `P(F, x*)` for each.\n",
        "    *   **Outputs:** For affective polarization, a scalar score. For issue salience, a DataFrame showing the relationship between `alpha` and `P(F, x*)`.\n",
        "    *   **Role in Research Pipeline:** These functions provide computational validation for the theoretical mechanisms discussed in the paper (Propositions 3 and 4). They demonstrate how the `P(F, x*)` measure connects to other important concepts like affective polarization and the salience of divisive issues.\n",
        "    *   **Equations Implemented:**\n",
        "        *   Affective Polarization: $A(F) = \\sum_{i: x_i < x^*} w_i \\cdot g(|x_i - m_R|) + \\sum_{i: x_i \\ge x^*} w_i \\cdot g(|x_i - m_L|)$\n",
        "        *   Issue Salience: $x = (1-\\alpha)c + \\alpha d$\n",
        "\n",
        "--\n",
        "\n",
        "#### **Task 12 & 14: Master Orchestrators**\n",
        "\n",
        "*   **Callables:** `run_polarization_pipeline`, `run_robustness_analysis`, `execute_full_research_project`\n",
        "    *   **Inputs:** The raw `anes_df` and the master `params` dictionary.\n",
        "    *   **Processes:** These are high-level orchestrators that execute the entire analytical workflow. `run_polarization_pipeline` runs the baseline analysis from Task 0 to 11. `run_robustness_analysis` repeatedly calls the pipeline under different assumptions (e.g., weighted vs. unweighted). `execute_full_research_project` calls both of these and the reporting function (`generate_report_visuals`) to produce the final, complete project output.\n",
        "    *   **Outputs:** A single, comprehensive, hierarchically structured dictionary containing all data, results, analyses, and report assets generated by the entire project.\n",
        "    *   **Role in Research Pipeline:** These functions encapsulate the entire research project into a single, reproducible, and automated workflow. They represent the final, professional-grade \"run\" command for the entire analysis, ensuring that the research can be executed from start to finish in a single, deterministic step.\n",
        "\n",
        "\n",
        "### Usage\n",
        "\n",
        "\n",
        "### **User Guide: Deploying the End-to-End Polarization Analysis Pipeline**\n",
        "\n",
        "This section provides a practical, step-by-step guide to utilizing the `execute_full_research_project` function. This function is the single entry point to the entire analytical library, designed for robust, reproducible, and comprehensive analysis of voter polarization.\n",
        "\n",
        "#### **Step 1: Data Acquisition and Preparation**\n",
        "\n",
        "The pipeline requires a single, consolidated `pandas.DataFrame` as its primary data input. This DataFrame must be prepared *before* calling the main function and must adhere to a strict schema.\n",
        "\n",
        "*   **Action:** The analyst must first acquire the necessary survey data (e.g., from the ANES Data Center). The data from multiple years and waves should be merged into one file.\n",
        "*   **Schema:** The resulting DataFrame must contain the following columns with these exact names:\n",
        "    *   `respondent_id`: A unique identifier for each respondent.\n",
        "    *   `year`: A string or object representing the survey wave (e.g., `'2004a'`, `'2016b'`).\n",
        "    *   `left_right`: The respondent's self-placement on the 0-10 left-right scale.\n",
        "    *   `liberal_conservative`: The respondent's self-placement on the 1-7 liberal-conservative scale.\n",
        "    *   `weight`: The full-sample survey weight provided by ANES for that specific survey wave.\n",
        "\n",
        "**Code Snippet: Creating a Mock DataFrame**\n",
        "\n",
        "For demonstration purposes, we will construct a small, mock `DataFrame` that conforms to this schema. In a real-world scenario, this `anes_df` would be the result of loading and merging actual ANES `.dta` or `.csv` files.\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# In a real application, this DataFrame would be loaded from a file.\n",
        "# df = pd.read_csv(\"path/to/your/merged_anes_data.csv\")\n",
        "# For this example, we create a mock DataFrame.\n",
        "\n",
        "data = {\n",
        "    'respondent_id': [f'R_{i}' for i in range(100)],\n",
        "    'year': np.random.choice(['2012', '2016a', '2016b', '2020'], 100),\n",
        "    'left_right': np.random.choice(list(range(11)) + [98], 100), # Include a missing code\n",
        "    'liberal_conservative': np.random.choice(list(range(1, 8)) + [99], 100), # Include a missing code\n",
        "    'weight': np.random.uniform(0.5, 2.5, 100)\n",
        "}\n",
        "anes_df = pd.DataFrame(data)\n",
        "\n",
        "print(\"Sample of the prepared input DataFrame:\")\n",
        "print(anes_df.head())\n",
        "```\n",
        "\n",
        "#### **Step 2: Constructing the Master Parameters Dictionary**\n",
        "\n",
        "The `execute_full_research_project` function is controlled by a single, comprehensive parameters dictionary. This object encapsulates every tunable setting for the entire analysis, ensuring perfect reproducibility. We will construct this dictionary section by section, using the default values specified in the original project brief.\n",
        "\n",
        "**Code Snippet: Defining the `params` Dictionary**\n",
        "\n",
        "```python\n",
        "# Initialize the master parameters dictionary.\n",
        "params = {}\n",
        "\n",
        "# --- Central Points of Interest (x*) ---\n",
        "# These are the points around which polarization will be measured for each scale.\n",
        "params['central_points_params'] = {\n",
        "    \"left_right_x_stars\": [1, 2, 3, 4, 5, 6, 7, 8, 9], # Note: Excludes boundaries 0 and 10\n",
        "    \"liberal_conservative_x_stars\": [2, 3, 4, 5, 6] # Note: Excludes boundaries 1 and 7\n",
        "}\n",
        "\n",
        "# --- Policy Space Boundaries ---\n",
        "# These define the theoretical min and max of each ideological scale.\n",
        "params['boundaries_params'] = {\n",
        "    \"left_right_boundaries\": {'min': 0, 'max': 10},\n",
        "    \"liberal_conservative_boundaries\": {'min': 1, 'max': 7}\n",
        "}\n",
        "\n",
        "# --- Integration Parameters ---\n",
        "# Defines the numerical method for the P(F,x*) calculation.\n",
        "# Note: The current implementation is optimized for 'trapezoidal' on discrete data.\n",
        "params['integration_params'] = {\n",
        "    'method': 'trapezoidal',\n",
        "    'num_points': 1000\n",
        "}\n",
        "\n",
        "# --- Cleavage Finder Parameters ---\n",
        "# Defines the time periods to scan for the maximum increase in polarization.\n",
        "params['cleavage_finder_params'] = {\n",
        "    'time_points': [('2016a', '2020')], # Using years available in our mock data\n",
        "    'potential_x_stars': params['central_points_params']\n",
        "}\n",
        "\n",
        "# --- Analysis-Specific Parameters ---\n",
        "# Defines midpoints for partitioning temporal plots and centrist definitions.\n",
        "params['midpoints'] = {\n",
        "    'left_right': 5,\n",
        "    'liberal_conservative': 4\n",
        "}\n",
        "params['centrist_definitions'] = {\n",
        "    'left_right': [4, 5, 6],\n",
        "    'liberal_conservative': [3, 4, 5]\n",
        "}\n",
        "params['election_years_for_analysis'] = [2016] # Using year available in mock data\n",
        "\n",
        "# --- Theoretical Extension Parameters ---\n",
        "# Defines parameters for the issue salience simulation.\n",
        "params['salience_simulation_params'] = {\n",
        "    'salience_alphas': [0.1, 0.3, 0.5, 0.7, 0.9],\n",
        "    'common_value_dist': {'low': 4.8, 'high': 5.2},\n",
        "    'divisive_issue_dist': {'low': 0, 'high': 10},\n",
        "    'polarization_params': {\n",
        "        'x_star': 5, # The x* to calculate polarization around in the simulation\n",
        "        'boundaries': params['boundaries_params']['left_right_boundaries']\n",
        "    },\n",
        "    'random_seed': 42 # For reproducibility\n",
        "}\n",
        "\n",
        "# --- Optional: ANES Missing Value Codes ---\n",
        "# This can be customized if different codes are used in the data.\n",
        "params['missing_value_map'] = {\n",
        "    'left_right': [98, 99],\n",
        "    'liberal_conservative': [98, 99]\n",
        "}\n",
        "\n",
        "print(\"\\nMaster parameters dictionary constructed successfully.\")\n",
        "```\n",
        "\n",
        "#### **Step 3: Executing the Pipeline and Inspecting Results**\n",
        "\n",
        "With the input `DataFrame` and the `params` dictionary prepared, the entire research project can be executed with a single function call. The function will print its progress through the various stages of the analysis.\n",
        "\n",
        "**Code Snippet: Running the Master Orchestrator**\n",
        "\n",
        "```python\n",
        "# First, ensure the full library of functions is loaded in your environment.\n",
        "# from ginzburg_polarization_model_python_implementation import execute_full_research_project\n",
        "\n",
        "# Execute the entire research project.\n",
        "# This single call runs validation, cleaning, all calculations, analyses,\n",
        "# robustness checks, and reporting.\n",
        "project_results = execute_full_research_project(\n",
        "    anes_df=anes_df,\n",
        "    params=params\n",
        ")\n",
        "```\n",
        "\n",
        "The returned `project_results` object is a deeply nested dictionary containing every artifact generated during the run. An analyst can now programmatically access any piece of the analysis for inspection, custom plotting, or further work.\n",
        "\n",
        "**Code Snippet: Accessing and Inspecting Key Outputs**\n",
        "\n",
        "```python\n",
        "# --- Inspecting the Main Analysis Results ---\n",
        "print(\"\\n--- Example: Inspecting Key Outputs from the Main Analysis ---\")\n",
        "\n",
        "# Access the main results dictionary\n",
        "main_analysis_results = project_results['main_analysis']['results']\n",
        "\n",
        "# 1. View the summary of the cleavage point analysis\n",
        "print(\"\\nCleavage Point Analysis Summary:\")\n",
        "cleavage_summary = main_analysis_results['cleavage_analysis']\n",
        "print(cleavage_summary)\n",
        "\n",
        "# 2. View the summary of the comparative analysis (correlation)\n",
        "print(\"\\nCorrelation Summary (Traditional vs. Flexible Measures):\")\n",
        "correlation_summary = main_analysis_results['comparative_framework']['correlation_summary']\n",
        "print(correlation_summary)\n",
        "\n",
        "# 3. Access a specific plot (e.g., temporal trends for the left-right scale)\n",
        "report_assets = project_results['main_analysis']['report_assets']\n",
        "temporal_plot_lr = report_assets['plots']['temporal_trends_left_right']\n",
        "# To display the plot in a Jupyter environment:\n",
        "# temporal_plot_lr.show()\n",
        "# To save the plot to a file:\n",
        "# temporal_plot_lr.savefig(\"temporal_trends_left_right.png\", dpi=300)\n",
        "print(\"\\nTemporal trend plot for 'left_right' scale has been generated.\")\n",
        "\n",
        "# --- Inspecting the Robustness Analysis Results ---\n",
        "print(\"\\n--- Example: Comparing Robustness Check Results ---\")\n",
        "\n",
        "# Compare the cleavage point found in the weighted vs. unweighted scenarios\n",
        "try:\n",
        "    weighted_cleavage = project_results['robustness_analysis']['weighted']['cleavage_analysis']\n",
        "    unweighted_cleavage = project_results['robustness_analysis']['unweighted']['cleavage_analysis']\n",
        "\n",
        "    print(\"\\nCleavage points from 'weighted' analysis:\")\n",
        "    print(weighted_cleavage[['cleavage_point', 'relative_position']])\n",
        "\n",
        "    print(\"\\nCleavage points from 'unweighted' analysis:\")\n",
        "    print(unweighted_cleavage[['cleavage_point', 'relative_position']])\n",
        "except KeyError:\n",
        "    print(\"\\nRobustness check for one or more scenarios may have failed.\")\n",
        "\n",
        "```\n",
        "This example provides a complete, end-to-end workflow, demonstrating how a user can prepare their data, configure the analysis, execute the entire pipeline with one command, and access the structured, high-value outputs for interpretation.\n",
        "\n"
      ],
      "metadata": {
        "id": "sLQzyY2MCO9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 0: Parameter Validation\n",
        "\n",
        "def _validate_dataframe(\n",
        "    df: pd.DataFrame,\n",
        "    expected_cols: Set[str],\n",
        "    valid_years: Set[Any]\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Validates the structure and content of the input ANES DataFrame.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame to validate.\n",
        "        expected_cols (Set[str]): A set of required column names.\n",
        "        valid_years (Set[Any]): A set of permissible values for the 'year' column.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the DataFrame is empty, missing required columns, or contains\n",
        "                    invalid years or non-positive weights.\n",
        "        TypeError: If columns that should be numeric are not.\n",
        "    \"\"\"\n",
        "    # Step 1.1: Check if the DataFrame is empty.\n",
        "    # An empty DataFrame cannot be processed.\n",
        "    if df.empty:\n",
        "        # Raise an error indicating the DataFrame is empty.\n",
        "        raise ValueError(\"Input DataFrame 'anes_df' cannot be empty.\")\n",
        "\n",
        "    # Step 1.2: Check for the presence of all required columns.\n",
        "    # The analysis depends on a specific set of columns.\n",
        "    actual_cols = set(df.columns)\n",
        "    # Find the set of columns that are missing from the DataFrame.\n",
        "    missing_cols = expected_cols - actual_cols\n",
        "    # If any columns are missing, raise an error.\n",
        "    if missing_cols:\n",
        "        # Raise an error listing the missing columns.\n",
        "        raise ValueError(\n",
        "            f\"Input DataFrame 'anes_df' is missing required columns: {sorted(list(missing_cols))}\"\n",
        "        )\n",
        "\n",
        "    # Step 1.3: Validate the 'year' column against a set of allowed survey waves.\n",
        "    # The analysis is specific to certain ANES survey years.\n",
        "    unique_years = set(df['year'].unique())\n",
        "    # Identify any years present in the data that are not in the allowed list.\n",
        "    invalid_years = unique_years - valid_years\n",
        "    # If invalid years are found, raise an error.\n",
        "    if invalid_years:\n",
        "        # Raise an error listing the unexpected years.\n",
        "        raise ValueError(\n",
        "            f\"Input DataFrame 'anes_df' contains invalid or unexpected years: {sorted(list(invalid_years))}. \"\n",
        "            f\"Valid years are: {sorted(list(valid_years))}\"\n",
        "        )\n",
        "\n",
        "    # Step 1.4: Validate data types for numeric columns.\n",
        "    # Ideological scales and weights must be numeric for calculations.\n",
        "    for col in ['left_right', 'liberal_conservative', 'weight']:\n",
        "        # Check if the column's data type is numeric.\n",
        "        if not pd.api.types.is_numeric_dtype(df[col]):\n",
        "            # If not numeric, raise a TypeError.\n",
        "            raise TypeError(\n",
        "                f\"Column '{col}' in 'anes_df' must be of a numeric type, but found {df[col].dtype}.\"\n",
        "            )\n",
        "\n",
        "    # Step 1.5: Ensure all weight values are positive.\n",
        "    # Weights must be strictly positive for valid weighted statistical calculations.\n",
        "    if (df['weight'] <= 0).any():\n",
        "        # Raise an error if any weight is zero or negative.\n",
        "        raise ValueError(\"Column 'weight' in 'anes_df' must contain only positive values.\")\n",
        "\n",
        "def _validate_boundaries(\n",
        "    boundaries_params: Dict[str, Dict[str, float]]\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Validates the policy space boundaries dictionary.\n",
        "\n",
        "    Args:\n",
        "        boundaries_params (Dict[str, Dict[str, float]]): The boundaries dictionary.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the structure is incorrect or if min >= max.\n",
        "        TypeError: If boundary values are not numeric.\n",
        "    \"\"\"\n",
        "    # Step 3.1: Define the expected structure.\n",
        "    # The dictionary must contain boundaries for both ideological scales.\n",
        "    expected_scales = {'left_right_boundaries', 'liberal_conservative_boundaries'}\n",
        "    # If the top-level keys do not match expectations, raise an error.\n",
        "    if not expected_scales.issubset(boundaries_params.keys()):\n",
        "        # Raise an error indicating the missing scale definitions.\n",
        "        raise ValueError(f\"Boundary parameters missing keys. Expected: {expected_scales}\")\n",
        "\n",
        "    # Step 3.2: Iterate through each scale's boundaries to validate them.\n",
        "    for scale_name, bounds in boundaries_params.items():\n",
        "        # Check for the presence of 'min' and 'max' keys.\n",
        "        if not {'min', 'max'}.issubset(bounds.keys()):\n",
        "            # Raise an error if keys are missing for a scale.\n",
        "            raise ValueError(f\"Boundary definition for '{scale_name}' is missing 'min' or 'max' key.\")\n",
        "\n",
        "        # Extract min and max values.\n",
        "        min_val, max_val = bounds['min'], bounds['max']\n",
        "\n",
        "        # Check if min and max are numeric.\n",
        "        if not all(isinstance(v, (int, float)) for v in [min_val, max_val]):\n",
        "            # Raise a TypeError if values are not numeric.\n",
        "            raise TypeError(f\"Boundary values for '{scale_name}' must be numeric.\")\n",
        "\n",
        "        # Step 3.3: Enforce the logical constraint that min must be less than max.\n",
        "        # The policy space must have a positive width.\n",
        "        if min_val >= max_val:\n",
        "            # Raise an error if the condition is violated.\n",
        "            raise ValueError(\n",
        "                f\"Invalid boundaries for '{scale_name}': 'min' ({min_val}) must be strictly less than 'max' ({max_val}).\"\n",
        "            )\n",
        "\n",
        "def _validate_central_points(\n",
        "    central_points_params: Dict[str, List[float]],\n",
        "    boundaries_params: Dict[str, Dict[str, float]]\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Validates the central points of interest (x_stars) dictionary.\n",
        "\n",
        "    Args:\n",
        "        central_points_params (Dict[str, List[float]]): The central points dictionary.\n",
        "        boundaries_params (Dict[str, Dict[str, float]]): The validated boundaries dictionary.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If structure is incorrect, values are out of bounds, not sorted, or not unique.\n",
        "        TypeError: If values are not lists of numbers.\n",
        "    \"\"\"\n",
        "    # Step 2.1: Define the expected structure.\n",
        "    # The dictionary must contain x_star lists for both ideological scales.\n",
        "    expected_scales = {'left_right_x_stars', 'liberal_conservative_x_stars'}\n",
        "    # If the top-level keys do not match expectations, raise an error.\n",
        "    if not expected_scales.issubset(central_points_params.keys()):\n",
        "        # Raise an error indicating the missing x_star definitions.\n",
        "        raise ValueError(f\"Central points parameters missing keys. Expected: {expected_scales}\")\n",
        "\n",
        "    # Step 2.2: Iterate through each scale's x_star list to validate it.\n",
        "    for scale_name, x_stars in central_points_params.items():\n",
        "        # Check if the value is a list.\n",
        "        if not isinstance(x_stars, list):\n",
        "            # Raise a TypeError if it's not a list.\n",
        "            raise TypeError(f\"Central points for '{scale_name}' must be a list.\")\n",
        "        # Check if the list is empty.\n",
        "        if not x_stars:\n",
        "            # Raise a ValueError if the list is empty.\n",
        "            raise ValueError(f\"Central points list for '{scale_name}' cannot be empty.\")\n",
        "\n",
        "        # Convert to a NumPy array for efficient validation.\n",
        "        x_stars_arr = np.array(x_stars)\n",
        "\n",
        "        # Step 2.3: Check if all values are within the policy space boundaries.\n",
        "        # The corresponding boundary key is derived from the x_star key name.\n",
        "        boundary_key = scale_name.replace('_x_stars', '_boundaries')\n",
        "        # Get the min and max bounds for the current scale.\n",
        "        min_bound = boundaries_params[boundary_key]['min']\n",
        "        max_bound = boundaries_params[boundary_key]['max']\n",
        "        # Check if any x_star is outside the [min, max] range.\n",
        "        if np.any(x_stars_arr < min_bound) or np.any(x_stars_arr > max_bound):\n",
        "            # Raise an error specifying the valid range.\n",
        "            raise ValueError(\n",
        "                f\"Central points for '{scale_name}' are out of bounds. \"\n",
        "                f\"All values must be within [{min_bound}, {max_bound}].\"\n",
        "            )\n",
        "\n",
        "        # Step 2.4: Check for uniqueness and ascending order.\n",
        "        # The difference between consecutive elements must always be positive.\n",
        "        if not np.all(np.diff(x_stars_arr) > 0):\n",
        "            # Raise an error if the list is not strictly sorted.\n",
        "            raise ValueError(\n",
        "                f\"Central points list for '{scale_name}' must be unique and sorted in ascending order.\"\n",
        "            )\n",
        "\n",
        "def _validate_integration_params(\n",
        "    integration_params: Dict[str, Any]\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Validates the numerical integration parameters.\n",
        "\n",
        "    Args:\n",
        "        integration_params (Dict[str, Any]): The integration parameters dictionary.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If structure is incorrect, method is unsupported, or num_points is invalid.\n",
        "        TypeError: If num_points is not an integer.\n",
        "    \"\"\"\n",
        "    # Step 4.1: Check for required keys.\n",
        "    if not {'method', 'num_points'}.issubset(integration_params.keys()):\n",
        "        # Raise an error if keys are missing.\n",
        "        raise ValueError(\"Integration parameters must contain 'method' and 'num_points' keys.\")\n",
        "\n",
        "    # Step 4.2: Validate the integration method.\n",
        "    # For this project, only 'trapezoidal' is specified, but this allows for future extension.\n",
        "    supported_methods = {'trapezoidal'}\n",
        "    # Check if the specified method is in the set of supported methods.\n",
        "    if integration_params['method'] not in supported_methods:\n",
        "        # Raise an error listing the supported methods.\n",
        "        raise ValueError(\n",
        "            f\"Unsupported integration method '{integration_params['method']}'. \"\n",
        "            f\"Supported methods are: {supported_methods}\"\n",
        "        )\n",
        "\n",
        "    # Step 4.3: Validate the number of points for discretization.\n",
        "    num_points = integration_params['num_points']\n",
        "    # Check if num_points is an integer.\n",
        "    if not isinstance(num_points, int):\n",
        "        # Raise a TypeError if it's not an integer.\n",
        "        raise TypeError(f\"'num_points' must be an integer, but found {type(num_points)}.\")\n",
        "    # Check if num_points is positive and sufficiently large.\n",
        "    if num_points <= 1:\n",
        "        # Raise a ValueError for invalid number of points.\n",
        "        raise ValueError(f\"'num_points' must be an integer greater than 1, but found {num_points}.\")\n",
        "\n",
        "def _validate_cleavage_finder_params(\n",
        "    cleavage_params: Dict[str, Any],\n",
        "    df_years: Set[Any]\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Validates the cleavage point finder parameters.\n",
        "\n",
        "    Args:\n",
        "        cleavage_params (Dict[str, Any]): The cleavage finder parameters dictionary.\n",
        "        df_years (Set[Any]): The set of unique years available in the input DataFrame.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If structure is incorrect, time points are invalid or not in data.\n",
        "        TypeError: If time_points is not a list of tuples.\n",
        "    \"\"\"\n",
        "    # Step 5.1: Check for required keys.\n",
        "    if not {'time_points', 'potential_x_stars'}.issubset(cleavage_params.keys()):\n",
        "        # Raise an error if keys are missing.\n",
        "        raise ValueError(\"Cleavage finder parameters must contain 'time_points' and 'potential_x_stars' keys.\")\n",
        "\n",
        "    # Step 5.2: Validate the 'time_points' list.\n",
        "    time_points = cleavage_params['time_points']\n",
        "    # Check if it's a list.\n",
        "    if not isinstance(time_points, list):\n",
        "        # Raise a TypeError if it's not a list.\n",
        "        raise TypeError(\"'time_points' must be a list of tuples.\")\n",
        "\n",
        "    # A set to collect all years required for the cleavage analysis.\n",
        "    required_years = set()\n",
        "    # Iterate through each time period tuple.\n",
        "    for period in time_points:\n",
        "        # Check if each item is a tuple of length 2.\n",
        "        if not isinstance(period, tuple) or len(period) != 2:\n",
        "            # Raise a ValueError for malformed periods.\n",
        "            raise ValueError(f\"Each item in 'time_points' must be a tuple of length 2, but found {period}.\")\n",
        "\n",
        "        # Unpack the start and end year.\n",
        "        t1, t2 = period\n",
        "        # Step 5.3: Check for chronological order.\n",
        "        if t1 >= t2:\n",
        "            # Raise a ValueError if the period is not chronological.\n",
        "            raise ValueError(f\"Time periods must be chronological (t1 < t2), but found ({t1}, {t2}).\")\n",
        "\n",
        "        # Add the years to the set of required years.\n",
        "        required_years.add(t1)\n",
        "        required_years.add(t2)\n",
        "\n",
        "    # Step 5.4: Cross-validate required years against years available in the data.\n",
        "    # Find which of the required years are missing from the DataFrame.\n",
        "    missing_from_df = required_years - df_years\n",
        "    # If any required years are not in the data, the analysis cannot proceed.\n",
        "    if missing_from_df:\n",
        "        # Raise an error listing the missing years.\n",
        "        raise ValueError(\n",
        "            f\"Cleavage analysis requires years that are not present in the DataFrame: {sorted(list(missing_from_df))}. \"\n",
        "            f\"Available years: {sorted(list(df_years))}\"\n",
        "        )\n",
        "\n",
        "def validate_parameters(\n",
        "    anes_df: pd.DataFrame,\n",
        "    central_points_params: Dict[str, List[float]],\n",
        "    boundaries_params: Dict[str, Dict[str, float]],\n",
        "    integration_params: Dict[str, Any],\n",
        "    cleavage_finder_params: Dict[str, Any]\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Performs a comprehensive validation of all input parameters for the polarization analysis.\n",
        "\n",
        "    This function serves as a single entry point for validating all inputs before any\n",
        "    computation begins, ensuring data integrity and parameter sanity. It follows the\n",
        "    \"fail-fast\" principle, raising descriptive errors immediately upon finding an issue.\n",
        "\n",
        "    Args:\n",
        "        anes_df (pd.DataFrame):\n",
        "            The primary DataFrame containing ANES survey data. Expected columns are\n",
        "            ['respondent_id', 'year', 'left_right', 'liberal_conservative', 'weight'].\n",
        "        central_points_params (Dict[str, List[float]]):\n",
        "            A dictionary specifying the central points (x*) for analysis on each scale.\n",
        "            Example: {'left_right_x_stars': [0, ..., 10], 'liberal_conservative_x_stars': [1, ..., 7]}\n",
        "        boundaries_params (Dict[str, Dict[str, float]]):\n",
        "            A dictionary defining the min and max boundaries of the policy space for each scale.\n",
        "            Example: {'left_right_boundaries': {'min': 0, 'max': 10}, ...}\n",
        "        integration_params (Dict[str, Any]):\n",
        "            Parameters for numerical integration.\n",
        "            Example: {'method': 'trapezoidal', 'num_points': 1000}\n",
        "        cleavage_finder_params (Dict[str, Any]):\n",
        "            Parameters for the cleavage point finder algorithm, including time periods.\n",
        "            Example: {'time_points': [(2012, 2016), ...], ...}\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If any parameter has an invalid value or logical inconsistency.\n",
        "        TypeError: If any parameter has an incorrect data type.\n",
        "    \"\"\"\n",
        "    # Define expected columns and valid years for the DataFrame validation.\n",
        "    expected_cols = {'respondent_id', 'year', 'left_right', 'liberal_conservative', 'weight'}\n",
        "    valid_years = {'2000', '2004a', '2004b', '2008', '2012', '2016a', '2016b', '2020'}\n",
        "\n",
        "    # Task 0, Step 1: Validate the main DataFrame.\n",
        "    _validate_dataframe(anes_df, expected_cols, valid_years)\n",
        "\n",
        "    # Task 0, Step 3: Validate policy space boundaries. This must be done before\n",
        "    # validating central points, as they depend on the boundaries.\n",
        "    _validate_boundaries(boundaries_params)\n",
        "\n",
        "    # Task 0, Step 2: Validate central points, using the now-validated boundaries.\n",
        "    _validate_central_points(central_points_params, boundaries_params)\n",
        "\n",
        "    # Task 0, Step 4: Validate integration parameters.\n",
        "    _validate_integration_params(integration_params)\n",
        "\n",
        "    # Task 0, Step 5: Validate cleavage finder parameters, cross-referencing with the DataFrame.\n",
        "    df_years = set(anes_df['year'].unique())\n",
        "    _validate_cleavage_finder_params(cleavage_finder_params, df_years)\n"
      ],
      "metadata": {
        "id": "btLPxY8SCQjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1: Data Cleansing\n",
        "\n",
        "def clean_anes_data(\n",
        "    anes_df: pd.DataFrame,\n",
        "    missing_value_map: Dict[str, List[Any]] = None\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Cleans the raw ANES DataFrame by handling infinite values, mapping specific\n",
        "    missing value codes to NaN, and dropping rows with missing essential data.\n",
        "\n",
        "    This function executes a rigorous, multi-step cleansing process crucial for\n",
        "    preparing survey data for quantitative analysis. It ensures that the\n",
        "    resulting DataFrame is free of non-substantive values in the core\n",
        "    analytical columns, making it ready for preprocessing and modeling.\n",
        "\n",
        "    The process is as follows:\n",
        "    1.  A copy of the input DataFrame is created to prevent side effects.\n",
        "    2.  Infinite values (np.inf, -np.inf) across the entire DataFrame are\n",
        "        replaced with np.nan. A warning is issued if any are found.\n",
        "    3.  User-defined missing value codes (e.g., 98 for \"Don't Know\", 99 for\n",
        "        \"Refused\") are replaced with np.nan for specified ideological columns.\n",
        "    4.  Rows containing any np.nan in the critical columns ('left_right',\n",
        "        'liberal_conservative', 'weight') are dropped (listwise deletion).\n",
        "    5.  The number of rows removed is reported to the user for transparency.\n",
        "    6.  The index of the cleaned DataFrame is reset to ensure it is contiguous.\n",
        "    7.  A final validation asserts that the critical columns are free of nulls.\n",
        "\n",
        "    Args:\n",
        "        anes_df (pd.DataFrame):\n",
        "            The raw input DataFrame from ANES. It is expected to have undergone\n",
        "            initial validation via the `validate_parameters` function.\n",
        "        missing_value_map (Dict[str, List[Any]], optional):\n",
        "            A dictionary mapping column names to a list of values that should be\n",
        "            treated as missing. If None, a default map for common ANES missing\n",
        "            codes ([98, 99]) is used for the ideological columns.\n",
        "            Example: {'left_right': [98, 99], 'liberal_conservative': [97, 98, 99]}\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame:\n",
        "            A new, cleaned DataFrame with no infinite values or missing data in\n",
        "            the core analytical columns.\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If 'anes_df' is not a pandas DataFrame.\n",
        "        ValueError: If 'anes_df' is empty.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Ensure the primary input is a pandas DataFrame.\n",
        "    if not isinstance(anes_df, pd.DataFrame):\n",
        "        # Raise a TypeError if the input is not of the expected type.\n",
        "        raise TypeError(\"Input 'anes_df' must be a pandas DataFrame.\")\n",
        "\n",
        "    # Ensure the DataFrame is not empty before proceeding.\n",
        "    if anes_df.empty:\n",
        "        # Raise a ValueError as cleansing an empty DataFrame is not meaningful.\n",
        "        raise ValueError(\"Input 'anes_df' cannot be empty.\")\n",
        "\n",
        "    # --- Data Cleansing Pipeline ---\n",
        "    # Create a copy to avoid modifying the original DataFrame in place. This is a\n",
        "    # critical best practice for robust data pipelines.\n",
        "    df_cleaned = anes_df.copy()\n",
        "\n",
        "    # Store the original number of rows for final reporting.\n",
        "    initial_rows = len(df_cleaned)\n",
        "\n",
        "    # --- Step 1: Identify and Replace Infinite Values ---\n",
        "    # Check for the presence of infinite values, which can result from data\n",
        "    # loading errors or upstream calculations.\n",
        "    if np.isinf(df_cleaned.select_dtypes(include=np.number)).any().any():\n",
        "        # Issue a warning to the user that infinite values were found and are being replaced.\n",
        "        # This alerts them to potential upstream data quality issues.\n",
        "        warnings.warn(\n",
        "            \"Infinite values found in the DataFrame. Replacing with np.nan.\",\n",
        "            UserWarning\n",
        "        )\n",
        "        # Replace all occurrences of numpy.inf and -numpy.inf with numpy.nan.\n",
        "        df_cleaned.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "    # --- Step 2: Handle Missing Value Codes Specific to ANES Surveys ---\n",
        "    # If no specific map is provided, use a default for the ideological scales.\n",
        "    # This provides sensible default behavior while allowing for customization.\n",
        "    if missing_value_map is None:\n",
        "        # Default map targets common ANES codes for \"Don't Know\" and \"Refused\".\n",
        "        missing_value_map = {\n",
        "            'left_right': [98, 99],\n",
        "            'liberal_conservative': [98, 99]\n",
        "        }\n",
        "\n",
        "    # Apply the replacements defined in the map.\n",
        "    # This converts survey-specific codes into the standard np.nan representation.\n",
        "    df_cleaned.replace(missing_value_map, np.nan, inplace=True)\n",
        "\n",
        "    # --- Step 3: Drop Rows with Missing Data in Critical Columns ---\n",
        "    # Define the columns that are essential for the polarization analysis.\n",
        "    # A row is only useful if it has valid data for both scales and a weight.\n",
        "    critical_cols = ['left_right', 'liberal_conservative', 'weight']\n",
        "\n",
        "    # Drop any row that has a null value in any of the critical columns.\n",
        "    # This is a listwise deletion strategy focused on the core analytical variables.\n",
        "    df_cleaned.dropna(subset=critical_cols, inplace=True)\n",
        "\n",
        "    # --- Step 4: Post-Cleansing Validation and Reporting ---\n",
        "    # Calculate the number of rows that were dropped during the process.\n",
        "    rows_dropped = initial_rows - len(df_cleaned)\n",
        "\n",
        "    # Report the outcome to the user for transparency and reproducibility.\n",
        "    print(\n",
        "        f\"Data cleansing complete. \"\n",
        "        f\"Removed {rows_dropped} of {initial_rows} rows \"\n",
        "        f\"({rows_dropped / initial_rows:.2%}) due to missing data in critical columns.\"\n",
        "    )\n",
        "\n",
        "    # If the entire DataFrame becomes empty after cleansing, raise an error.\n",
        "    if df_cleaned.empty:\n",
        "        raise ValueError(\n",
        "            \"All rows were removed during data cleansing. \"\n",
        "            \"Check data quality and missing value definitions.\"\n",
        "        )\n",
        "\n",
        "    # Reset the index of the DataFrame to be a clean, contiguous sequence from 0.\n",
        "    # This is crucial for preventing indexing errors in subsequent processing steps.\n",
        "    df_cleaned.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    # Final assertion: Verify that the critical columns now contain no null values.\n",
        "    # This is a self-check to guarantee the function's post-conditions are met.\n",
        "    assert df_cleaned[critical_cols].isnull().sum().sum() == 0, \\\n",
        "        \"Post-cleansing validation failed: Null values remain in critical columns.\"\n",
        "\n",
        "    # Return the fully cleaned and validated DataFrame.\n",
        "    return df_cleaned\n"
      ],
      "metadata": {
        "id": "CmKBuR6VDcM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: Data Preprocessing\n",
        "\n",
        "def preprocess_for_polarization(\n",
        "    cleaned_df: pd.DataFrame\n",
        ") -> Dict[Any, Dict[str, Dict[str, np.ndarray]]]:\n",
        "    \"\"\"\n",
        "    Preprocesses cleaned ANES data to create a hierarchical, analysis-ready structure.\n",
        "\n",
        "    This function transforms a flat, cleaned DataFrame into a nested dictionary\n",
        "    optimized for the polarization calculations. The structure is:\n",
        "    {year: {scale: {'positions': array, 'weights': array, 'cdf': DataFrame}}}.\n",
        "    This pre-computation avoids repeated filtering and sorting during the main\n",
        "    analysis loop, significantly improving performance.\n",
        "\n",
        "    The process for each year and ideological scale ('left_right', 'liberal_conservative') is:\n",
        "    1.  Normalizes survey weights within each year to sum to 1.0, creating\n",
        "        probabilistic weights.\n",
        "    2.  For each scale, sorts the data by ideological position. This is a\n",
        "        prerequisite for correct CDF construction.\n",
        "    3.  Constructs a weighted Probability Mass Function (PMF) by summing the\n",
        "        normalized weights at each unique ideological position.\n",
        "    4.  Constructs the weighted Cumulative Distribution Function (CDF) by\n",
        "        computing the cumulative sum of the PMF.\n",
        "    5.  Performs rigorous quality assurance checks on each generated CDF to\n",
        "        ensure it is mathematically valid (monotonic, bounded [0, 1]).\n",
        "    6.  Stores the sorted positions, corresponding weights, and the CDF\n",
        "        (as a DataFrame mapping positions to values) in the final nested dictionary.\n",
        "\n",
        "    Args:\n",
        "        cleaned_df (pd.DataFrame):\n",
        "            A DataFrame that has been processed by `clean_anes_data`. It must\n",
        "            contain the columns 'year', 'weight', 'left_right', and\n",
        "            'liberal_conservative'.\n",
        "\n",
        "    Returns:\n",
        "        Dict[Any, Dict[str, Dict[str, np.ndarray]]]:\n",
        "            A nested dictionary where keys are years, then scales. The innermost\n",
        "            dictionary contains NumPy arrays for 'positions' and 'weights', and\n",
        "            a pandas DataFrame for the 'cdf' (index=position, column='cdf_value').\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the input DataFrame is empty or if a year's data becomes\n",
        "                    empty after scale-specific processing.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Ensure the input DataFrame is not empty.\n",
        "    if cleaned_df.empty:\n",
        "        # Raise an error as preprocessing an empty DataFrame is not possible.\n",
        "        raise ValueError(\"Input 'cleaned_df' cannot be empty.\")\n",
        "\n",
        "    # --- Step 1: Apply ANES Survey Weights (Normalization) ---\n",
        "    # Create a copy to ensure the original DataFrame is not modified.\n",
        "    df = cleaned_df.copy()\n",
        "    # For each group (year), transform the 'weight' column by dividing each\n",
        "    # weight by the sum of weights for that year. This creates 'normalized_weight'.\n",
        "    # w_normalized_i = w_i / sum(w_j) for all j in the same year.\n",
        "    df['normalized_weight'] = df.groupby('year')['weight'].transform(lambda x: x / x.sum())\n",
        "\n",
        "    # --- Step 2 & 3: Create Hierarchical Structure and Construct CDFs ---\n",
        "    # Initialize the top-level dictionary to store the structured results.\n",
        "    preprocessed_data = {}\n",
        "    # Define the ideological scales to be processed.\n",
        "    scales = ['left_right', 'liberal_conservative']\n",
        "\n",
        "    # Iterate through each unique year present in the DataFrame.\n",
        "    for year in df['year'].unique():\n",
        "        # Initialize the dictionary for the current year.\n",
        "        preprocessed_data[year] = {}\n",
        "        # Get the subset of the DataFrame corresponding to the current year.\n",
        "        year_df = df[df['year'] == year]\n",
        "\n",
        "        # Iterate through each ideological scale.\n",
        "        for scale in scales:\n",
        "            # For each scale, select the relevant columns and drop any rows that\n",
        "            # might have a NaN value for this specific scale.\n",
        "            scale_df = year_df[[scale, 'normalized_weight']].dropna(subset=[scale])\n",
        "\n",
        "            # If a specific year-scale combination has no data, skip it.\n",
        "            if scale_df.empty:\n",
        "                # This can happen if a survey year did not include a particular question.\n",
        "                continue\n",
        "\n",
        "            # Sort the data by the ideological position. This is essential for\n",
        "            # creating the PMF and CDF correctly.\n",
        "            scale_df_sorted = scale_df.sort_values(by=scale)\n",
        "\n",
        "            # Construct the weighted Probability Mass Function (PMF).\n",
        "            # Group by the unique ideological positions and sum their normalized weights.\n",
        "            # This correctly handles ties in positions.\n",
        "            pmf = scale_df_sorted.groupby(scale)['normalized_weight'].sum().reset_index()\n",
        "            pmf.columns = ['position', 'pmf_value']\n",
        "\n",
        "            # Construct the weighted Cumulative Distribution Function (CDF).\n",
        "            # F(x) = sum_{i: x_i <= x} w_normalized_i\n",
        "            # This is achieved by taking the cumulative sum of the PMF values.\n",
        "            pmf['cdf_value'] = pmf['pmf_value'].cumsum()\n",
        "\n",
        "            # Create the final CDF DataFrame, indexed by position for fast lookups.\n",
        "            cdf_df = pmf[['position', 'cdf_value']].set_index('position')\n",
        "\n",
        "            # --- Step 4: Data Quality Assurance for the CDF ---\n",
        "            # Retrieve the computed CDF values as a NumPy array.\n",
        "            cdf_values = cdf_df['cdf_value'].values\n",
        "            # Assert that the CDF is monotonically non-decreasing.\n",
        "            # The difference between consecutive elements must be >= 0.\n",
        "            assert np.all(np.diff(cdf_values) >= -1e-9), \\\n",
        "                f\"CDF for {year}-{scale} is not monotonic.\"\n",
        "            # Assert that the last value of the CDF is approximately 1.0.\n",
        "            # np.isclose handles potential floating-point inaccuracies.\n",
        "            assert np.isclose(cdf_values[-1], 1.0), \\\n",
        "                f\"CDF for {year}-{scale} does not sum to 1.0 (is {cdf_values[-1]}).\"\n",
        "\n",
        "            # Store the final, validated results in the hierarchical dictionary.\n",
        "            # We store the sorted positions and weights from the original sorted\n",
        "            # DataFrame, and the computed CDF map.\n",
        "            preprocessed_data[year][scale] = {\n",
        "                'positions': scale_df_sorted[scale].values,\n",
        "                'weights': scale_df_sorted['normalized_weight'].values,\n",
        "                'cdf': cdf_df\n",
        "            }\n",
        "\n",
        "    # Return the final, structured, and validated data object.\n",
        "    return preprocessed_data\n"
      ],
      "metadata": {
        "id": "1Z4LiGWrEOzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3: Implement the Polarization Measure Function\n",
        "\n",
        "def calculate_polarization_index(\n",
        "    cdf_df: pd.DataFrame,\n",
        "    x_star: float,\n",
        "    boundaries: Dict[str, float]\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Calculates the flexible measure of voter polarization, P(F, x*).\n",
        "\n",
        "    This function provides a precise, numerically stable implementation of the\n",
        "    polarization index defined in Definition 2 of Ginzburg's paper. The index\n",
        "    measures the dispersion of an electorate's ideological distribution (F)\n",
        "    around a specified central point (x*).\n",
        "\n",
        "    The formula is:\n",
        "    P(F, x*) := [A / (x* - min{X})] - [B / (max{X} - x*)] + 1\n",
        "    where:\n",
        "    A = Integral from min{X} to x* of F(x) dx\n",
        "    B = Integral from x* to max{X} of F(x) dx\n",
        "\n",
        "    For a discrete empirical CDF (a step function), the integrals are calculated\n",
        "    exactly as the sum of the areas of rectangles under the CDF curve.\n",
        "\n",
        "    Args:\n",
        "        cdf_df (pd.DataFrame):\n",
        "            A DataFrame representing the Cumulative Distribution Function (CDF).\n",
        "            The index must contain the unique ideological positions, and a column\n",
        "            named 'cdf_value' must contain the corresponding CDF values. This\n",
        "            is generated by the `preprocess_for_polarization` function.\n",
        "        x_star (float):\n",
        "            The central point of interest around which polarization is measured.\n",
        "        boundaries (Dict[str, float]):\n",
        "            A dictionary containing the theoretical minimum and maximum of the\n",
        "            policy space, e.g., {'min': 0, 'max': 10}.\n",
        "\n",
        "    Returns:\n",
        "        float:\n",
        "            The calculated polarization index P(F, x*), a value in [0, 1].\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If x_star is not strictly within the policy space boundaries.\n",
        "        KeyError: If 'boundaries' or 'cdf_df' have incorrect structure.\n",
        "    \"\"\"\n",
        "    # --- Step 3: Handle Edge Cases and Numerical Stability (Input Validation) ---\n",
        "    # Extract min and max boundaries for the policy space.\n",
        "    min_X = boundaries['min']\n",
        "    max_X = boundaries['max']\n",
        "\n",
        "    # The polarization measure is mathematically undefined at the boundaries.\n",
        "    # Ensure x_star is strictly within the interior of the policy space.\n",
        "    if not (min_X < x_star < max_X):\n",
        "        # Raise an error if x_star is at or outside the boundaries.\n",
        "        raise ValueError(\n",
        "            f\"x_star ({x_star}) must be strictly between the boundaries \"\n",
        "            f\"min ({min_X}) and max ({max_X}).\"\n",
        "        )\n",
        "\n",
        "    # --- Step 2: Implement Numerical Integration for Step Functions ---\n",
        "    # Extract the unique positions and CDF values into NumPy arrays for performance.\n",
        "    positions = cdf_df.index.values\n",
        "    cdf_values = cdf_df['cdf_value'].values\n",
        "\n",
        "    # To correctly calculate the area under the step function, we need to define\n",
        "    # the intervals. We create a full set of points including the boundaries.\n",
        "    # This ensures the integration covers the entire range from min_X to max_X.\n",
        "    full_pos = np.union1d(np.union1d(positions, [x_star]), [min_X, max_X])\n",
        "\n",
        "    # We need the CDF value at each point in our full set of positions.\n",
        "    # We use np.searchsorted to efficiently find the CDF value for each point.\n",
        "    # 'right' gives F(x), the probability of being <= x.\n",
        "    # We find the index in the original `positions` for each point in `full_pos`.\n",
        "    indices = np.searchsorted(positions, full_pos, side='right')\n",
        "    # We create an array of corresponding CDF values. For points before the first\n",
        "    # actual position, the CDF is 0.\n",
        "    full_cdf_values = np.concatenate(([0], cdf_values))[indices]\n",
        "\n",
        "    # Calculate the widths of the rectangles under the CDF curve.\n",
        "    # This is the distance between each consecutive point in our full set.\n",
        "    widths = np.diff(full_pos)\n",
        "\n",
        "    # The heights of the rectangles are the CDF values at the start of each interval.\n",
        "    heights = full_cdf_values[:-1]\n",
        "\n",
        "    # The area of each rectangle is height * width.\n",
        "    areas = heights * widths\n",
        "\n",
        "    # Now, we sum these areas over the correct domains for integrals A and B.\n",
        "    # Get the midpoints of our integration intervals.\n",
        "    midpoints = (full_pos[:-1] + full_pos[1:]) / 2\n",
        "\n",
        "    # Integral A: Sum of areas where the interval midpoint is less than x_star.\n",
        "    # This corresponds to ∫_{min{X}}^{x*} F(x) dx\n",
        "    integral_A = np.sum(areas[midpoints < x_star])\n",
        "\n",
        "    # Integral B: Sum of areas where the interval midpoint is greater than or equal to x_star.\n",
        "    # This corresponds to ∫_{x*}^{max{X}} F(x) dx\n",
        "    integral_B = np.sum(areas[midpoints >= x_star])\n",
        "\n",
        "    # --- Step 1: Implement Core Polarization Formula P(F,x*) ---\n",
        "    # Calculate the first normalized term of the equation.\n",
        "    term1 = integral_A / (x_star - min_X)\n",
        "\n",
        "    # Calculate the second normalized term of the equation.\n",
        "    term2 = integral_B / (max_X - x_star)\n",
        "\n",
        "    # Combine the terms according to the formula from Definition 2.\n",
        "    polarization_value = term1 - term2 + 1\n",
        "\n",
        "    # --- Step 4: Validate Mathematical Properties of P(F,x*) ---\n",
        "    # According to Proposition 2, the result must be in the range [0, 1].\n",
        "    # This assertion acts as a final self-check on the calculation's correctness.\n",
        "    # We use a small tolerance to account for potential floating-point inaccuracies.\n",
        "    assert -1e-9 <= polarization_value <= 1 + 1e-9, \\\n",
        "        f\"Calculation resulted in P={polarization_value}, which is outside the theoretical [0, 1] range.\"\n",
        "\n",
        "    # Clip the result to the [0, 1] range to handle any minor floating point overflow.\n",
        "    polarization_value = np.clip(polarization_value, 0, 1)\n",
        "\n",
        "    # Return the final, validated polarization index.\n",
        "    return polarization_value\n"
      ],
      "metadata": {
        "id": "P7iVFIeCEuue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4: Compute Polarization Measures\n",
        "\n",
        "def compute_all_polarization_measures(\n",
        "    preprocessed_data: Dict[Any, Dict[str, Dict[str, Union[np.ndarray, pd.DataFrame]]]],\n",
        "    central_points_params: Dict[str, List[float]],\n",
        "    boundaries_params: Dict[str, Dict[str, float]]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes the polarization index P(F, x*) for all years, scales, and central points.\n",
        "\n",
        "    This function orchestrates the core analysis by systematically iterating through\n",
        "    every combination of survey year, ideological scale, and specified central\n",
        "    point (x*). It calls the `calculate_polarization_index` function for each\n",
        "    combination and aggregates the results into a single, structured DataFrame.\n",
        "\n",
        "    The output is a DataFrame with a MultiIndex ('year', 'scale', 'x_star'),\n",
        "    which is the canonical data structure for the subsequent temporal and\n",
        "    comparative analysis tasks.\n",
        "\n",
        "    Args:\n",
        "        preprocessed_data (Dict):\n",
        "            The hierarchical data structure created by `preprocess_for_polarization`.\n",
        "            Contains the pre-computed CDFs for each year and scale.\n",
        "        central_points_params (Dict[str, List[float]]):\n",
        "            A dictionary specifying the central points (x*) for analysis on each scale.\n",
        "        boundaries_params (Dict[str, Dict[str, float]]):\n",
        "            A dictionary defining the min and max boundaries of the policy space.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame:\n",
        "            A DataFrame containing all computed polarization values, indexed by\n",
        "            year, scale, and x_star. The columns are ['polarization_value'].\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If a year-scale combination specified by the parameters\n",
        "                    is not found in the preprocessed data.\n",
        "    \"\"\"\n",
        "    # --- Step 1: Vectorize (Iterate) Polarization Calculation ---\n",
        "    # Initialize a list to store the results as dictionaries. This is an\n",
        "    # efficient way to build up the data before converting to a DataFrame.\n",
        "    results_list = []\n",
        "\n",
        "    # Iterate through each year in the preprocessed data.\n",
        "    for year, year_data in preprocessed_data.items():\n",
        "        # Iterate through each ideological scale ('left_right', 'liberal_conservative').\n",
        "        for scale, scale_data in year_data.items():\n",
        "            # Determine the correct keys for the parameter dictionaries based on the scale.\n",
        "            x_stars_key = f\"{scale}_x_stars\"\n",
        "            boundaries_key = f\"{scale}_boundaries\"\n",
        "\n",
        "            # Retrieve the list of x* values and the boundaries for the current scale.\n",
        "            x_stars = central_points_params[x_stars_key]\n",
        "            boundaries = boundaries_params[boundaries_key]\n",
        "\n",
        "            # Retrieve the pre-computed CDF for this specific year and scale.\n",
        "            cdf_df = scale_data['cdf']\n",
        "\n",
        "            # Iterate through each central point (x*) for the current scale.\n",
        "            for x_star in x_stars:\n",
        "                try:\n",
        "                    # Call the core function from Task 3 to compute the index.\n",
        "                    polarization_value = calculate_polarization_index(\n",
        "                        cdf_df=cdf_df,\n",
        "                        x_star=x_star,\n",
        "                        boundaries=boundaries\n",
        "                    )\n",
        "\n",
        "                    # Append the result as a dictionary to our list.\n",
        "                    results_list.append({\n",
        "                        'year': year,\n",
        "                        'scale': scale,\n",
        "                        'x_star': x_star,\n",
        "                        'polarization_value': polarization_value\n",
        "                    })\n",
        "                except ValueError as e:\n",
        "                    # If calculate_polarization_index raises a ValueError (e.g., x* is\n",
        "                    # on a boundary), we print a warning and skip that point.\n",
        "                    # This makes the process robust to invalid x* points in the config.\n",
        "                    warnings.warn(\n",
        "                        f\"Skipping calculation for {year}-{scale} at x*={x_star}: {e}\",\n",
        "                        UserWarning\n",
        "                    )\n",
        "                    continue\n",
        "\n",
        "    # If no results were generated, it indicates a problem.\n",
        "    if not results_list:\n",
        "        raise ValueError(\"No polarization measures could be computed. Check input data and parameters.\")\n",
        "\n",
        "    # --- Step 2: Store Results in Structured Multi-Dimensional Format ---\n",
        "    # Convert the list of dictionaries into a pandas DataFrame.\n",
        "    # This is a highly efficient and standard way to create a DataFrame from collected data.\n",
        "    results_df = pd.DataFrame(results_list)\n",
        "\n",
        "    # Set the hierarchical MultiIndex for efficient slicing and analysis.\n",
        "    # This is the optimal structure for this kind of multi-dimensional panel data.\n",
        "    results_df.set_index(['year', 'scale', 'x_star'], inplace=True)\n",
        "\n",
        "    # Sort the index for performance and clean presentation.\n",
        "    results_df.sort_index(inplace=True)\n",
        "\n",
        "    # --- Step 3: Implement Quality Control and Validation ---\n",
        "    # Assert that there are no null values in the final results. This would\n",
        "    # indicate a failure during computation that was not caught.\n",
        "    assert results_df['polarization_value'].isnull().sum() == 0, \\\n",
        "        \"Validation failed: Found NaN values in the final results DataFrame.\"\n",
        "\n",
        "    # Assert that all computed values fall within the theoretical [0, 1] range.\n",
        "    # This is a final sanity check on the entire batch of computations.\n",
        "    assert results_df['polarization_value'].between(0, 1).all(), \\\n",
        "        \"Validation failed: Polarization values found outside the theoretical [0, 1] range.\"\n",
        "\n",
        "    # Return the final, structured, and validated DataFrame of polarization measures.\n",
        "    return results_df\n"
      ],
      "metadata": {
        "id": "5DEpSfuxFbhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 5: Temporal Analysis\n",
        "\n",
        "def _convert_year_to_numeric(year_series: pd.Series) -> pd.Series:\n",
        "    \"\"\"Converts a Series of ANES year strings to a numeric format for plotting.\"\"\"\n",
        "    # Replaces 'a' (pre-election) with .0 and 'b' (post-election) with .5\n",
        "    return (\n",
        "        year_series.astype(str)\n",
        "        .str.replace('a', '.0')\n",
        "        .str.replace('b', '.5')\n",
        "        .astype(float)\n",
        "    )\n",
        "\n",
        "def prepare_temporal_analysis(\n",
        "    polarization_results: pd.DataFrame,\n",
        "    midpoints: Dict[str, float]\n",
        ") -> Dict[str, Dict[str, Dict[str, pd.DataFrame]]]:\n",
        "    \"\"\"\n",
        "    Prepares polarization data for temporal analysis and visualization.\n",
        "\n",
        "    This function takes the computed polarization measures and transforms them\n",
        "    into structures optimized for time-series plotting and analysis. It replicates\n",
        "    the analytical setup used in the source paper (e.g., Figure 1), which\n",
        "    involves partitioning the data into left/liberal and right/conservative\n",
        "    panels based on the position of the central point (x*) relative to a scale\n",
        "    midpoint.\n",
        "\n",
        "    The process is as follows:\n",
        "    1.  Converts the 'year' index level to a numeric format to enable\n",
        "        chronological plotting (e.g., '2004a' -> 2004.0, '2004b' -> 2004.5).\n",
        "    2.  For each ideological scale ('left_right', 'liberal_conservative'):\n",
        "        a.  Pivots the data into a \"wide\" format where the index is the\n",
        "            numeric year and columns correspond to each x*.\n",
        "        b.  Partitions this wide DataFrame into two: one for x* values at or\n",
        "            below the midpoint ('left_panel') and one for x* values above\n",
        "            the midpoint ('right_panel').\n",
        "    3.  For each of these partitioned DataFrames, it also creates a \"long\"\n",
        "        format version, which is the standard input for many modern plotting\n",
        "        libraries like Seaborn.\n",
        "    4.  Returns a nested dictionary containing all four partitioned DataFrames\n",
        "        in both wide and long formats, ready for direct use in visualization.\n",
        "\n",
        "    Args:\n",
        "        polarization_results (pd.DataFrame):\n",
        "            The MultiIndex DataFrame of polarization values from\n",
        "            `compute_all_polarization_measures`.\n",
        "        midpoints (Dict[str, float]):\n",
        "            A dictionary defining the midpoint for each scale used for partitioning.\n",
        "            Example: {'left_right': 5, 'liberal_conservative': 4}\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Dict[str, Dict[str, pd.DataFrame]]]:\n",
        "            A nested dictionary structured as:\n",
        "            {scale: {'left_panel' | 'right_panel': {'wide' | 'long': DataFrame}}}\n",
        "            This contains all the necessary data views for temporal plotting.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(polarization_results, pd.DataFrame) or not isinstance(polarization_results.index, pd.MultiIndex):\n",
        "        raise TypeError(\"`polarization_results` must be a DataFrame with a MultiIndex.\")\n",
        "    if not {'year', 'scale', 'x_star'}.issubset(polarization_results.index.names):\n",
        "        raise ValueError(\"`polarization_results` must have index levels 'year', 'scale', and 'x_star'.\")\n",
        "\n",
        "    # --- Step 1: Organize Time Series Data ---\n",
        "    # Create a working copy of the results DataFrame.\n",
        "    df = polarization_results.copy()\n",
        "\n",
        "    # Convert the 'year' level of the index to a numeric format for plotting.\n",
        "    # This is a critical step for treating the time axis correctly.\n",
        "    numeric_year_index = _convert_year_to_numeric(df.index.get_level_values('year'))\n",
        "    # Replace the original categorical year index with the new numeric one.\n",
        "    df.index = pd.MultiIndex.from_arrays(\n",
        "        [numeric_year_index, df.index.get_level_values('scale'), df.index.get_level_values('x_star')],\n",
        "        names=['year_numeric', 'scale', 'x_star']\n",
        "    )\n",
        "\n",
        "    # Initialize the final output dictionary.\n",
        "    analysis_data = {}\n",
        "    # Get the unique scales from the data, e.g., ['left_right', 'liberal_conservative'].\n",
        "    scales = df.index.get_level_values('scale').unique()\n",
        "\n",
        "    # Process each scale independently.\n",
        "    for scale in scales:\n",
        "        # Initialize the dictionary for the current scale.\n",
        "        analysis_data[scale] = {}\n",
        "        # Select the data for the current scale. The result is a Series\n",
        "        # with a MultiIndex of ('year_numeric', 'x_star').\n",
        "        scale_series = df.xs(scale, level='scale')['polarization_value']\n",
        "\n",
        "        # Unstack the 'x_star' level to create a wide-format DataFrame.\n",
        "        # The index will be 'year_numeric', and columns will be the x* values.\n",
        "        # This structure is ideal for analyzing multiple time series together.\n",
        "        wide_df = scale_series.unstack(level='x_star')\n",
        "\n",
        "        # --- Step 2: Implement Left-Center-Right Partitioning Logic ---\n",
        "        # Retrieve the midpoint for the current scale from the parameters.\n",
        "        midpoint = midpoints[scale]\n",
        "\n",
        "        # Define the two panels based on the midpoint.\n",
        "        panels = {\n",
        "            'left_panel': wide_df.loc[:, wide_df.columns <= midpoint],\n",
        "            'right_panel': wide_df.loc[:, wide_df.columns > midpoint]\n",
        "        }\n",
        "\n",
        "        # --- Step 3 & 4: Prepare Data Structures for Visualization ---\n",
        "        # Process each of the two panels (left and right).\n",
        "        for panel_name, panel_df_wide in panels.items():\n",
        "            # If a panel is empty (e.g., no x* values > midpoint), skip it.\n",
        "            if panel_df_wide.empty:\n",
        "                continue\n",
        "\n",
        "            # \"Melt\" the wide-format DataFrame to create a long-format version.\n",
        "            # This is the standard \"tidy\" data format preferred by Seaborn.\n",
        "            panel_df_long = panel_df_wide.reset_index().melt(\n",
        "                id_vars='year_numeric',\n",
        "                var_name='x_star',\n",
        "                value_name='polarization_value'\n",
        "            )\n",
        "\n",
        "            # Store both the wide and long formats in the final output structure.\n",
        "            # This provides maximum flexibility for different plotting/analysis needs.\n",
        "            analysis_data[scale][panel_name] = {\n",
        "                'wide': panel_df_wide,\n",
        "                'long': panel_df_long\n",
        "            }\n",
        "\n",
        "    # Return the fully structured dictionary ready for visualization.\n",
        "    return analysis_data\n"
      ],
      "metadata": {
        "id": "kZZaJCyeGFAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 6: Election Year Analysis\n",
        "\n",
        "def analyze_election_year_effects(\n",
        "    polarization_results: pd.DataFrame,\n",
        "    election_years: List[int]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Analyzes the change in polarization before and after specified election years.\n",
        "\n",
        "    This function isolates the short-term impact of an election by comparing\n",
        "    polarization measures from pre-election ('a' wave) and post-election\n",
        "    ('b' wave) surveys. It calculates the absolute change,\n",
        "    ΔP = P_post - P_pre, for each ideological scale and central point (x*).\n",
        "\n",
        "    The process is as follows:\n",
        "    1.  Iterates through a list of specified base election years (e.g., 2004, 2016).\n",
        "    2.  For each year, it extracts the pre-election (e.g., '2004a') and\n",
        "        post-election (e.g., '2004b') data from the main results DataFrame.\n",
        "    3.  It performs a vectorized subtraction to compute the change in polarization\n",
        "        for every (scale, x*) pair.\n",
        "    4.  The results (pre-election value, post-election value, and the change)\n",
        "        are consolidated into a single, tidy DataFrame with a MultiIndex of\n",
        "        (election_year, scale, x_star) for easy analysis and plotting.\n",
        "\n",
        "    Args:\n",
        "        polarization_results (pd.DataFrame):\n",
        "            The MultiIndex DataFrame of polarization values from\n",
        "            `compute_all_polarization_measures`. Must have 'year' as a level\n",
        "            in the index.\n",
        "        election_years (List[int]):\n",
        "            A list of integer years to analyze (e.g., [2004, 2016]). The function\n",
        "            will look for corresponding 'a' and 'b' waves.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame:\n",
        "            A DataFrame containing the pre- and post-election polarization values\n",
        "            and their difference. The index is ('election_year', 'scale', 'x_star').\n",
        "            Columns are ['polarization_pre', 'polarization_post', 'polarization_change'].\n",
        "            Returns an empty DataFrame if no valid election year pairs are found.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(polarization_results, pd.DataFrame) or not isinstance(polarization_results.index, pd.MultiIndex):\n",
        "        raise TypeError(\"`polarization_results` must be a DataFrame with a MultiIndex.\")\n",
        "    if 'year' not in polarization_results.index.names:\n",
        "        raise ValueError(\"`polarization_results` must have 'year' as an index level.\")\n",
        "\n",
        "    # A list to hold the results DataFrames for each election year.\n",
        "    all_election_results = []\n",
        "\n",
        "    # --- Step 1: Extract Pre- and Post-Election Data Pairs ---\n",
        "    # Iterate through each specified base election year.\n",
        "    for year_int in election_years:\n",
        "        # Construct the string identifiers for the pre ('a') and post ('b') waves.\n",
        "        pre_election_wave = f\"{year_int}a\"\n",
        "        post_election_wave = f\"{year_int}b\"\n",
        "\n",
        "        try:\n",
        "            # Extract the data for the pre-election wave.\n",
        "            # The .xs() method is efficient for selecting data from a specific\n",
        "            # level of a MultiIndex.\n",
        "            pre_df = polarization_results.xs(pre_election_wave, level='year')\n",
        "\n",
        "            # Extract the data for the post-election wave.\n",
        "            post_df = polarization_results.xs(post_election_wave, level='year')\n",
        "\n",
        "            # --- Step 2: Compute Pre-Post Polarization Differences ---\n",
        "            # Calculate the change: ΔP = P_post - P_pre.\n",
        "            # Pandas' vectorized subtraction automatically aligns the indices\n",
        "            # ('scale', 'x_star'), ensuring a correct element-wise calculation.\n",
        "            diff_df = post_df - pre_df\n",
        "\n",
        "            # --- Step 3 & 4: Consolidate and Structure Results ---\n",
        "            # Rename columns for clarity in the final merged DataFrame.\n",
        "            pre_df.rename(columns={'polarization_value': 'polarization_pre'}, inplace=True)\n",
        "            post_df.rename(columns={'polarization_value': 'polarization_post'}, inplace=True)\n",
        "            diff_df.rename(columns={'polarization_value': 'polarization_change'}, inplace=True)\n",
        "\n",
        "            # Concatenate the three DataFrames (pre, post, change) side-by-side.\n",
        "            # The result is a single DataFrame for the current election year.\n",
        "            year_results_df = pd.concat([pre_df, post_df, diff_df], axis=1)\n",
        "\n",
        "            # Add the integer election year as a new index level for organization.\n",
        "            year_results_df['election_year'] = year_int\n",
        "            year_results_df.set_index('election_year', append=True, inplace=True)\n",
        "\n",
        "            # Reorder index levels to the desired (election_year, scale, x_star) format.\n",
        "            year_results_df = year_results_df.reorder_levels(['election_year', 'scale', 'x_star'])\n",
        "\n",
        "            # Add the consolidated results for this year to our list.\n",
        "            all_election_results.append(year_results_df)\n",
        "\n",
        "        except KeyError:\n",
        "            # If either the 'a' or 'b' wave is not found for a year, issue a\n",
        "            # warning and skip that year. This makes the function robust.\n",
        "            warnings.warn(\n",
        "                f\"Could not find both pre-election ('{pre_election_wave}') and \"\n",
        "                f\"post-election ('{post_election_wave}') data. Skipping year {year_int}.\",\n",
        "                UserWarning\n",
        "            )\n",
        "            continue\n",
        "\n",
        "    # If the list of results is empty, return an empty DataFrame.\n",
        "    if not all_election_results:\n",
        "        warnings.warn(\"No valid election year pairs were found in the data. Returning an empty DataFrame.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Concatenate the results from all processed election years into a single DataFrame.\n",
        "    final_results_df = pd.concat(all_election_results)\n",
        "\n",
        "    # Sort the index for clean presentation and efficient lookups.\n",
        "    final_results_df.sort_index(inplace=True)\n",
        "\n",
        "    return final_results_df\n"
      ],
      "metadata": {
        "id": "SIvI2EbsGxRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 7: Implement Cleavage Point Finder Algorithm\n",
        "\n",
        "def find_cleavage_points(\n",
        "    polarization_results: pd.DataFrame,\n",
        "    cleavage_finder_params: Dict[str, Any],\n",
        "    zero_threshold: float = 1e-6\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Identifies ideological cleavage points by finding the x* with the maximum\n",
        "    percentage increase in polarization over specified time periods.\n",
        "\n",
        "    This function implements the \"Cleavage Point Finder\" algorithm, a form of\n",
        "    grid search. For each specified time period (t1, t2) and ideological scale,\n",
        "    it calculates the percentage change in P(F, x*) for all candidate x* values.\n",
        "    It then identifies the x* at which this increase is maximized, pinpointing\n",
        "    the primary ideological fault line for that period.\n",
        "\n",
        "    The algorithm is as follows:\n",
        "    1.  For each (t1, t2) period and scale, extract the polarization values.\n",
        "    2.  Calculate the absolute change (P_t2 - P_t1) and percentage change\n",
        "        (100 * (P_t2 - P_t1) / P_t1).\n",
        "    3.  To ensure numerical stability, the percentage change is only calculated\n",
        "        if the initial polarization P_t1 is above a small threshold.\n",
        "    4.  A deterministic tie-breaking rule is used: candidates are sorted first\n",
        "        by percentage change (descending), then by absolute change (descending).\n",
        "        The top result is selected as the cleavage point.\n",
        "    5.  Results are compiled into a summary DataFrame.\n",
        "\n",
        "    Args:\n",
        "        polarization_results (pd.DataFrame):\n",
        "            The MultiIndex DataFrame of polarization values from\n",
        "            `compute_all_polarization_measures`.\n",
        "        cleavage_finder_params (Dict[str, Any]):\n",
        "            A dictionary specifying the time periods and candidate x* values.\n",
        "            Expected keys: 'time_points' (List[Tuple]) and 'potential_x_stars' (Dict).\n",
        "        zero_threshold (float, optional):\n",
        "            The value below which an initial polarization P_t1 is considered\n",
        "            too small for a meaningful percentage change calculation. Defaults to 1e-6.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame:\n",
        "            A DataFrame summarizing the analysis, indexed by a string representation\n",
        "            of the time period and the scale. Columns include the identified\n",
        "            'cleavage_point' (the winning x*), 'max_perc_change', and the\n",
        "            'abs_change_at_cleavage'.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(polarization_results, pd.DataFrame) or not isinstance(polarization_results.index, pd.MultiIndex):\n",
        "        raise TypeError(\"`polarization_results` must be a DataFrame with a MultiIndex.\")\n",
        "    if not {'time_points', 'potential_x_stars'}.issubset(cleavage_finder_params.keys()):\n",
        "        raise ValueError(\"`cleavage_finder_params` is missing required keys.\")\n",
        "\n",
        "    # A list to store the final summary results for each search.\n",
        "    summary_results = []\n",
        "\n",
        "    # --- Step 1: Implement Core Grid Search Algorithm Structure ---\n",
        "    # Iterate through each specified time period tuple (t1, t2).\n",
        "    for t1, t2 in cleavage_finder_params['time_points']:\n",
        "        # Iterate through each ideological scale defined in the parameters.\n",
        "        for scale, x_stars in cleavage_finder_params['potential_x_stars'].items():\n",
        "            try:\n",
        "                # Extract polarization values for the start (t1) and end (t2) of the period.\n",
        "                # We select for the specific scale and the set of candidate x* values.\n",
        "                p_t1 = polarization_results.loc[(t1, scale, x_stars), 'polarization_value']\n",
        "                p_t2 = polarization_results.loc[(t2, scale, x_stars), 'polarization_value']\n",
        "\n",
        "                # Combine into a single DataFrame for vectorized calculations.\n",
        "                # The index is 'x_star'.\n",
        "                changes_df = pd.DataFrame({'p_t1': p_t1, 'p_t2': p_t2})\n",
        "\n",
        "                # Calculate the absolute change in polarization.\n",
        "                changes_df['abs_change'] = changes_df['p_t2'] - changes_df['p_t1']\n",
        "\n",
        "                # --- Step 2: Implement Robust Percentage Change Calculation ---\n",
        "                # To avoid division by zero or near-zero, we only calculate percentage\n",
        "                # change where the initial value p_t1 is above a threshold.\n",
        "                # Equation: 100 * (P_t2 - P_t1) / P_t1\n",
        "                changes_df['perc_change'] = np.where(\n",
        "                    changes_df['p_t1'] > zero_threshold,\n",
        "                    100 * changes_df['abs_change'] / changes_df['p_t1'],\n",
        "                    np.nan  # Assign NaN where p_t1 is too small for meaningful calculation.\n",
        "                )\n",
        "\n",
        "                # Drop any rows where percentage change could not be calculated.\n",
        "                valid_changes = changes_df.dropna(subset=['perc_change'])\n",
        "\n",
        "                if valid_changes.empty:\n",
        "                    warnings.warn(f\"No valid data to find cleavage point for {scale} in period {t1}-{t2}.\")\n",
        "                    continue\n",
        "\n",
        "                # --- Step 3: Implement Optimization and Tie-Breaking Logic ---\n",
        "                # Sort to find the best candidate.\n",
        "                # Primary sort key: percentage change (descending).\n",
        "                # Secondary sort key (tie-breaker): absolute change (descending).\n",
        "                sorted_candidates = valid_changes.sort_values(\n",
        "                    by=['perc_change', 'abs_change'], ascending=[False, False]\n",
        "                )\n",
        "\n",
        "                # The winning cleavage point is the index of the top row after sorting.\n",
        "                best_candidate = sorted_candidates.iloc[0]\n",
        "                cleavage_point = best_candidate.name\n",
        "\n",
        "                # --- Step 4: Store and Validate Algorithm Results ---\n",
        "                # Append the summary of this search to our results list.\n",
        "                summary_results.append({\n",
        "                    'time_period': f\"{t1}-{t2}\",\n",
        "                    'scale': scale,\n",
        "                    'cleavage_point': cleavage_point,\n",
        "                    'max_perc_change': best_candidate['perc_change'],\n",
        "                    'abs_change_at_cleavage': best_candidate['abs_change']\n",
        "                })\n",
        "\n",
        "            except KeyError:\n",
        "                warnings.warn(\n",
        "                    f\"Data not available for period {t1}-{t2} and scale '{scale}'. Skipping.\",\n",
        "                    UserWarning\n",
        "                )\n",
        "                continue\n",
        "\n",
        "    # If no results were generated, return an empty DataFrame.\n",
        "    if not summary_results:\n",
        "        warnings.warn(\"Cleavage point analysis yielded no results.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Convert the list of dictionaries to a final summary DataFrame.\n",
        "    summary_df = pd.DataFrame(summary_results)\n",
        "    # Set a clean, descriptive index.\n",
        "    summary_df.set_index(['time_period', 'scale'], inplace=True)\n",
        "\n",
        "    return summary_df\n"
      ],
      "metadata": {
        "id": "mauoVQN3Hcee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 8: Identify Cleavage Points\n",
        "\n",
        "def identify_and_analyze_cleavage_points(\n",
        "    polarization_results: pd.DataFrame,\n",
        "    cleavage_finder_params: Dict[str, Any],\n",
        "    midpoints: Dict[str, float],\n",
        "    zero_threshold: float = 1e-6\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Applies the cleavage finder algorithm and enriches the results with\n",
        "    contextual analysis of the cleavage points' location and significance.\n",
        "\n",
        "    This function serves as the primary application layer for Task 8. It first\n",
        "    calls the `find_cleavage_points` algorithm (from Task 7) to get the raw\n",
        "    results. It then performs further analysis on these results to provide\n",
        "    deeper insights, addressing the following:\n",
        "    1.  **Spatial Analysis:** It determines where each cleavage point lies\n",
        "        relative to its scale's midpoint (e.g., 'Left of Center').\n",
        "    2.  **Significance Heuristic:** It calculates the percentile rank of the\n",
        "        cleavage point's percentage increase relative to the increases at all\n",
        "        other candidate points. A high percentile suggests the cleavage is a\n",
        "        statistically prominent feature, not just a marginal winner.\n",
        "\n",
        "    Args:\n",
        "        polarization_results (pd.DataFrame):\n",
        "            The MultiIndex DataFrame of polarization values.\n",
        "        cleavage_finder_params (Dict[str, Any]):\n",
        "            Parameters specifying time periods and candidate x* values.\n",
        "        midpoints (Dict[str, float]):\n",
        "            A dictionary defining the midpoint for each scale.\n",
        "        zero_threshold (float, optional):\n",
        "            Threshold for meaningful percentage change calculation.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame:\n",
        "            An enriched DataFrame, indexed by ('time_period', 'scale'), with\n",
        "            detailed analysis of each identified cleavage point, including its\n",
        "            location, relative position, and percentile rank of its change.\n",
        "    \"\"\"\n",
        "    # --- Step 1: Apply Cleavage Finder to Specified Time Periods ---\n",
        "    # Call the core algorithm from Task 7 to get the initial results.\n",
        "    cleavage_summary_df = find_cleavage_points(\n",
        "        polarization_results=polarization_results,\n",
        "        cleavage_finder_params=cleavage_finder_params,\n",
        "        zero_threshold=zero_threshold\n",
        "    )\n",
        "\n",
        "    # If the core algorithm returns no results, exit gracefully.\n",
        "    if cleavage_summary_df.empty:\n",
        "        warnings.warn(\"Initial cleavage point search yielded no results. Cannot perform analysis.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # --- Step 2: Analyze Spatial Distribution of Cleavage Points ---\n",
        "    # Add the midpoint for each scale to the summary DataFrame for context.\n",
        "    cleavage_summary_df['scale_midpoint'] = cleavage_summary_df.index.get_level_values('scale').map(midpoints)\n",
        "\n",
        "    # Define a function to categorize the cleavage point's position.\n",
        "    def get_relative_position(row: pd.Series) -> str:\n",
        "        # Use np.sign to determine if the point is left, right, or at the center.\n",
        "        sign = np.sign(row['cleavage_point'] - row['scale_midpoint'])\n",
        "        if sign < 0:\n",
        "            return 'Left of Center'\n",
        "        elif sign > 0:\n",
        "            return 'Right of Center'\n",
        "        else:\n",
        "            return 'Center'\n",
        "\n",
        "    # Apply the function to create a new 'relative_position' column.\n",
        "    cleavage_summary_df['relative_position'] = cleavage_summary_df.apply(get_relative_position, axis=1)\n",
        "\n",
        "    # --- Step 3 & 4: Validate Significance and Interpret in Context ---\n",
        "    # To calculate the percentile, we need the distribution of *all* changes for each period.\n",
        "    # We must re-run the change calculation part of the algorithm.\n",
        "    percentiles = []\n",
        "    for (time_period, scale), row in cleavage_summary_df.iterrows():\n",
        "        t1_str, t2_str = time_period.split('-')\n",
        "        t1, t2 = int(t1_str), int(t2_str)\n",
        "\n",
        "        x_stars = cleavage_finder_params['potential_x_stars'][scale]\n",
        "\n",
        "        # Re-calculate the changes for this specific period-scale combination.\n",
        "        p_t1 = polarization_results.loc[(t1, scale, x_stars), 'polarization_value']\n",
        "        p_t2 = polarization_results.loc[(t2, scale, x_stars), 'polarization_value']\n",
        "        changes_df = pd.DataFrame({'p_t1': p_t1, 'p_t2': p_t2})\n",
        "        changes_df['perc_change'] = np.where(\n",
        "            changes_df['p_t1'] > zero_threshold,\n",
        "            100 * (changes_df['p_t2'] - changes_df['p_t1']) / changes_df['p_t1'],\n",
        "            np.nan\n",
        "        ).copy()\n",
        "\n",
        "        # Get the full distribution of valid percentage changes.\n",
        "        all_changes = changes_df['perc_change'].dropna().values\n",
        "        # Get the winning percentage change for the identified cleavage point.\n",
        "        winning_change = row['max_perc_change']\n",
        "\n",
        "        # Calculate the percentile rank of the winning change within its distribution.\n",
        "        # A value of 100 means it was the highest or tied for highest.\n",
        "        if len(all_changes) > 0:\n",
        "            percentile = percentileofscore(all_changes, winning_change, kind='weak')\n",
        "        else:\n",
        "            percentile = np.nan # Should not happen if cleavage was found.\n",
        "\n",
        "        percentiles.append(percentile)\n",
        "\n",
        "    # Add the calculated percentile rank as a new column.\n",
        "    # This provides a powerful heuristic for the \"significance\" or \"prominence\"\n",
        "    # of the identified cleavage point.\n",
        "    cleavage_summary_df['perc_change_percentile'] = percentiles\n",
        "\n",
        "    # Return the final, enriched DataFrame.\n",
        "    return cleavage_summary_df\n"
      ],
      "metadata": {
        "id": "QqR1zZKHIG-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 9: Compute Traditional Polarization Measures\n",
        "\n",
        "def _weighted_descriptive_stats(\n",
        "    values: pd.Series,\n",
        "    weights: pd.Series\n",
        ") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Calculates weighted mean, variance, and standard deviation for a Series.\n",
        "\n",
        "    Args:\n",
        "        values (pd.Series): A Series of data points (e.g., ideological positions).\n",
        "        weights (pd.Series): A Series of corresponding survey weights.\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: A Series containing the weighted 'mean', 'variance', and 'std_dev'.\n",
        "    \"\"\"\n",
        "    # Calculate the weighted average (mean).\n",
        "    # μ_w = Σ(w_i * x_i) / Σ(w_i)\n",
        "    weighted_mean = np.average(values, weights=weights)\n",
        "\n",
        "    # Calculate the weighted variance.\n",
        "    # Var_w = Σ(w_i * (x_i - μ_w)²) / Σ(w_i)\n",
        "    # Note: The denominator is the sum of weights, not N-1.\n",
        "    weighted_variance = np.average((values - weighted_mean)**2, weights=weights)\n",
        "\n",
        "    # Weighted standard deviation is the square root of the variance.\n",
        "    weighted_std_dev = np.sqrt(weighted_variance)\n",
        "\n",
        "    # Return the results as a pandas Series for easy integration.\n",
        "    return pd.Series({\n",
        "        'mean': weighted_mean,\n",
        "        'variance': weighted_variance,\n",
        "        'std_dev': weighted_std_dev\n",
        "    })\n",
        "\n",
        "def compute_traditional_measures(\n",
        "    cleaned_df: pd.DataFrame,\n",
        "    centrist_definitions: Dict[str, List[int]]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes traditional polarization measures for comparison.\n",
        "\n",
        "    This function calculates established, mean-centric and distribution-based\n",
        "    measures of polarization to serve as a baseline for evaluating the insights\n",
        "    from the flexible P(F, x*) index.\n",
        "\n",
        "    The computed measures for each year and ideological scale are:\n",
        "    1.  **Weighted Mean, Variance, and Standard Deviation:** These measure the\n",
        "        central tendency and dispersion of the entire distribution. Variance is a\n",
        "        classic, widely used polarization metric.\n",
        "    2.  **Share of Centrist Voters:** This measures the \"hollowing out\" of the\n",
        "        ideological middle by calculating the weighted proportion of respondents\n",
        "        who place themselves in a predefined centrist range.\n",
        "\n",
        "    Args:\n",
        "        cleaned_df (pd.DataFrame):\n",
        "            A DataFrame processed by `clean_anes_data`, containing columns 'year',\n",
        "            'weight', 'left_right', and 'liberal_conservative'.\n",
        "        centrist_definitions (Dict[str, List[int]]):\n",
        "            A dictionary defining the range of values considered \"centrist\" for\n",
        "            each scale. Example: {'left_right': [4, 5, 6], ...}\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame:\n",
        "            A DataFrame indexed by ('year', 'scale') containing the computed\n",
        "            traditional measures as columns.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if cleaned_df.empty:\n",
        "        raise ValueError(\"Input 'cleaned_df' cannot be empty.\")\n",
        "    if not isinstance(centrist_definitions, dict):\n",
        "        raise TypeError(\"`centrist_definitions` must be a dictionary.\")\n",
        "\n",
        "    # A list to store the results DataFrames from each scale.\n",
        "    all_measures = []\n",
        "    # Define the ideological scales to process.\n",
        "    scales = ['left_right', 'liberal_conservative']\n",
        "\n",
        "    for scale in scales:\n",
        "        # Create a working DataFrame for the current scale, dropping any\n",
        "        # potential NaNs in the scale or weight columns.\n",
        "        scale_df = cleaned_df[['year', scale, 'weight']].dropna()\n",
        "\n",
        "        # --- Step 1 & 3: Calculate Weighted Descriptive Statistics ---\n",
        "        # Group by year and apply the weighted stats helper function.\n",
        "        # This computes mean, variance, and std_dev for each year.\n",
        "        desc_stats = scale_df.groupby('year').apply(\n",
        "            lambda g: _weighted_descriptive_stats(g[scale], g['weight'])\n",
        "        )\n",
        "\n",
        "        # --- Step 2: Calculate Centrist Voter Shares ---\n",
        "        # Define a helper function to calculate the weighted share of centrists.\n",
        "        def get_centrist_share(group: pd.DataFrame) -> float:\n",
        "            # Filter the group to include only respondents in the centrist range.\n",
        "            centrist_mask = group[scale].isin(centrist_definitions[scale])\n",
        "            # Sum the weights of the centrist respondents.\n",
        "            centrist_weight_sum = group.loc[centrist_mask, 'weight'].sum()\n",
        "            # Sum the weights of all respondents in the group.\n",
        "            total_weight_sum = group['weight'].sum()\n",
        "            # The share is the ratio of the two sums.\n",
        "            return centrist_weight_sum / total_weight_sum if total_weight_sum > 0 else 0.0\n",
        "\n",
        "        # Group by year and apply the centrist share function.\n",
        "        centrist_share = scale_df.groupby('year').apply(get_centrist_share)\n",
        "        # Convert the resulting Series to a DataFrame with a descriptive column name.\n",
        "        centrist_share_df = centrist_share.to_frame(name='centrist_share')\n",
        "\n",
        "        # --- Step 4: Organize Measures for Comparative Analysis ---\n",
        "        # Join the descriptive statistics and the centrist share results.\n",
        "        # The 'year' index aligns the two DataFrames perfectly.\n",
        "        combined_measures = desc_stats.join(centrist_share_df)\n",
        "\n",
        "        # Add a 'scale' column to identify which scale these measures belong to.\n",
        "        combined_measures['scale'] = scale\n",
        "\n",
        "        # Append the combined results for this scale to our list.\n",
        "        all_measures.append(combined_measures)\n",
        "\n",
        "    # Concatenate the results from all scales into a single DataFrame.\n",
        "    final_df = pd.concat(all_measures)\n",
        "    # Set a clean MultiIndex of ('year', 'scale').\n",
        "    final_df.set_index('scale', append=True, inplace=True)\n",
        "    final_df = final_df.reorder_levels(['year', 'scale'])\n",
        "\n",
        "    # Sort the index for clean presentation and efficient lookups.\n",
        "    final_df.sort_index(inplace=True)\n",
        "\n",
        "    return final_df\n"
      ],
      "metadata": {
        "id": "rZmUvtfBJQaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 10: Comparative Analysis\n",
        "\n",
        "def create_comparative_analysis_framework(\n",
        "    polarization_results: pd.DataFrame,\n",
        "    traditional_measures: pd.DataFrame\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Creates a unified framework for comparing the flexible polarization index\n",
        "    against traditional measures.\n",
        "\n",
        "    This function is central to demonstrating the value of the P(F, x*) index.\n",
        "    It integrates the results from the flexible measure with traditional metrics\n",
        "    (like variance and centrist share) into a single, analysis-ready DataFrame.\n",
        "    It then performs a systematic correlation analysis to quantitatively identify\n",
        "    where the different measures tell similar stories and, more importantly, where\n",
        "    they diverge, thus highlighting the unique insights offered by the new index.\n",
        "\n",
        "    The process is as follows:\n",
        "    1.  Pivots the `polarization_results` DataFrame to a wide format where each\n",
        "        x* has its own column.\n",
        "    2.  Joins this wide DataFrame with the `traditional_measures` DataFrame,\n",
        "        aligning them by 'year' and 'scale'. This creates the master\n",
        "        comparative framework.\n",
        "    3.  For each scale, it computes the temporal correlation matrix between all\n",
        "        traditional measures and all P(F, x*) measures.\n",
        "    4.  From this matrix, it extracts a summary identifying, for each traditional\n",
        "        measure, which x* is most and least correlated with it.\n",
        "    5.  Returns a dictionary containing both the master unified DataFrame and\n",
        "        the concise correlation summary.\n",
        "\n",
        "    Args:\n",
        "        polarization_results (pd.DataFrame):\n",
        "            The MultiIndex DataFrame of P(F, x*) values from Task 4.\n",
        "        traditional_measures (pd.DataFrame):\n",
        "            The DataFrame of traditional measures from Task 9.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, pd.DataFrame]:\n",
        "            A dictionary with two keys:\n",
        "            'unified_data': The master DataFrame containing all measures.\n",
        "            'correlation_summary': A DataFrame summarizing the correlation\n",
        "                                   analysis.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not all(isinstance(df, pd.DataFrame) for df in [polarization_results, traditional_measures]):\n",
        "        raise TypeError(\"Both inputs must be pandas DataFrames.\")\n",
        "    if polarization_results.empty or traditional_measures.empty:\n",
        "        raise ValueError(\"Input DataFrames cannot be empty.\")\n",
        "\n",
        "    # --- Step 1: Create Integrated Comparison Framework ---\n",
        "    # Unstack the 'x_star' level to pivot the P(F, x*) results into a wide format.\n",
        "    # The index will be ('year', 'scale'), and columns will be the x* values.\n",
        "    p_results_wide = polarization_results['polarization_value'].unstack(level='x_star')\n",
        "    # Add a prefix to the column names to clearly identify them as P(F, x*) measures.\n",
        "    p_results_wide.columns = [f\"p_xstar_{col}\" for col in p_results_wide.columns]\n",
        "\n",
        "    # Join the wide-format P(F, x*) results with the traditional measures.\n",
        "    # The join is performed on the shared ('year', 'scale') index, ensuring perfect alignment.\n",
        "    unified_df = traditional_measures.join(p_results_wide)\n",
        "\n",
        "    # Drop any rows with NaNs that might result from an incomplete join, though\n",
        "    # this should not happen with correctly generated inputs.\n",
        "    unified_df.dropna(inplace=True)\n",
        "\n",
        "    if unified_df.empty:\n",
        "        raise ValueError(\"The join between polarization results and traditional measures yielded an empty DataFrame.\")\n",
        "\n",
        "    # --- Step 2 & 3: Identify Discrepancies and Quantify Value-Added ---\n",
        "    # Initialize a list to store the correlation summary results.\n",
        "    correlation_analysis_results = []\n",
        "    # Get the unique scales from the unified DataFrame's index.\n",
        "    scales = unified_df.index.get_level_values('scale').unique()\n",
        "    # Get the column names for the traditional measures.\n",
        "    trad_measure_cols = traditional_measures.columns.tolist()\n",
        "    # Get the column names for the P(F, x*) measures.\n",
        "    p_measure_cols = p_results_wide.columns.tolist()\n",
        "\n",
        "    # Perform the correlation analysis separately for each scale.\n",
        "    for scale in scales:\n",
        "        # Select the data for the current scale.\n",
        "        scale_df = unified_df.xs(scale, level='scale')\n",
        "\n",
        "        # Compute the full Pearson correlation matrix for all measures over time.\n",
        "        corr_matrix = scale_df.corr(method='pearson')\n",
        "\n",
        "        # We are interested in the correlations between traditional and P(F,x*) measures.\n",
        "        # Select the relevant block of the correlation matrix.\n",
        "        cross_corr = corr_matrix.loc[trad_measure_cols, p_measure_cols]\n",
        "\n",
        "        # For each traditional measure, find the P(F,x*) it correlates most/least with.\n",
        "        for trad_measure in trad_measure_cols:\n",
        "            # Get the correlation series for the current traditional measure.\n",
        "            corr_series = cross_corr.loc[trad_measure]\n",
        "\n",
        "            # Find the x* with the maximum correlation.\n",
        "            max_corr_col = corr_series.idxmax()\n",
        "            max_corr_val = corr_series.max()\n",
        "\n",
        "            # Find the x* with the minimum correlation.\n",
        "            min_corr_col = corr_series.idxmin()\n",
        "            min_corr_val = corr_series.min()\n",
        "\n",
        "            # Store the results. We extract the numeric x* value from the column name.\n",
        "            correlation_analysis_results.append({\n",
        "                'scale': scale,\n",
        "                'traditional_measure': trad_measure,\n",
        "                'most_correlated_x_star': int(max_corr_col.split('_')[-1]),\n",
        "                'correlation_max': max_corr_val,\n",
        "                'least_correlated_x_star': int(min_corr_col.split('_')[-1]),\n",
        "                'correlation_min': min_corr_val\n",
        "            })\n",
        "\n",
        "    # Convert the list of results into a structured DataFrame.\n",
        "    correlation_summary_df = pd.DataFrame(correlation_analysis_results)\n",
        "    # Set a clean index for the summary table.\n",
        "    correlation_summary_df.set_index(['scale', 'traditional_measure'], inplace=True)\n",
        "\n",
        "    # --- Step 4: Synthesize and Return the Framework ---\n",
        "    # The final output is a dictionary containing both the comprehensive\n",
        "    # unified dataset and the high-level correlation summary.\n",
        "    return {\n",
        "        'unified_data': unified_df,\n",
        "        'correlation_summary': correlation_summary_df\n",
        "    }\n"
      ],
      "metadata": {
        "id": "tQJMolxFJ6sL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 11: Theoretical Extensions\n",
        "\n",
        "def calculate_affective_polarization(\n",
        "    positions: np.ndarray,\n",
        "    weights: np.ndarray,\n",
        "    x_star: float,\n",
        "    g_func: Callable[[np.ndarray], np.ndarray]\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Calculates the overall affective polarization of an electorate.\n",
        "\n",
        "    This function implements the model from Proposition 3, where affective\n",
        "    polarization is the average animosity felt by members of one group towards\n",
        "    the opposing group. Animosity is an increasing function `g` of the\n",
        "    ideological distance to the mean of the other group.\n",
        "\n",
        "    The formula for the total weighted animosity is:\n",
        "    A(F) = Σ_{i: x_i < x*} w_i*g(|x_i-m_R|) + Σ_{i: x_i >= x*} w_i*g(|x_i-m_L|)\n",
        "\n",
        "    Args:\n",
        "        positions (np.ndarray): Array of ideological positions for all voters.\n",
        "        weights (np.ndarray): Array of corresponding normalized survey weights.\n",
        "        x_star (float): The ideological point that divides the electorate into\n",
        "                        a 'left' group (positions < x_star) and a 'right' group.\n",
        "        g_func (Callable[[np.ndarray], np.ndarray]):\n",
        "            A vectorized function representing animosity, g(distance). It should\n",
        "            take an array of distances and return an array of animosities.\n",
        "\n",
        "    Returns:\n",
        "        float: The aggregate affective polarization score for the electorate.\n",
        "    \"\"\"\n",
        "    # --- Step 1: Partition Electorate into Left and Right Groups ---\n",
        "    # Create boolean masks to identify members of the left and right groups.\n",
        "    left_mask = positions < x_star\n",
        "    right_mask = positions >= x_star\n",
        "\n",
        "    # If either group is empty, affective polarization is undefined.\n",
        "    if not np.any(left_mask) or not np.any(right_mask):\n",
        "        return np.nan\n",
        "\n",
        "    # --- Step 2: Calculate Group Mean Positions ---\n",
        "    # Calculate the weighted mean position of the left group (m_L).\n",
        "    m_L = np.average(positions[left_mask], weights=weights[left_mask])\n",
        "    # Calculate the weighted mean position of the right group (m_R).\n",
        "    m_R = np.average(positions[right_mask], weights=weights[right_mask])\n",
        "\n",
        "    # --- Step 3: Calculate Animosity for Each Group ---\n",
        "    # Animosity of the left group towards the right group.\n",
        "    # This is a weighted sum of the animosity of each left-group member.\n",
        "    # Distance for each left-group member is |x_i - m_R|.\n",
        "    distances_L_to_R = np.abs(positions[left_mask] - m_R)\n",
        "    animosity_L = np.sum(weights[left_mask] * g_func(distances_L_to_R))\n",
        "\n",
        "    # Animosity of the right group towards the left group.\n",
        "    # Distance for each right-group member is |x_i - m_L|.\n",
        "    distances_R_to_L = np.abs(positions[right_mask] - m_L)\n",
        "    animosity_R = np.sum(weights[right_mask] * g_func(distances_R_to_L))\n",
        "\n",
        "    # The total affective polarization is the sum of the animosities from both sides.\n",
        "    total_affective_polarization = animosity_L + animosity_R\n",
        "\n",
        "    return total_affective_polarization\n",
        "\n",
        "\n",
        "def simulate_issue_salience_effect(\n",
        "    salience_alphas: List[float],\n",
        "    common_value_dist: Dict[str, Any],\n",
        "    divisive_issue_dist: Dict[str, Any],\n",
        "    polarization_params: Dict[str, Any],\n",
        "    n_voters: int = 10000,\n",
        "    random_seed: Optional[int] = None\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Simulates the effect of issue salience on ideological polarization.\n",
        "\n",
        "    This function implements the model from Proposition 4, where a voter's\n",
        "    ideology `x` is a weighted average of a common-value position `c` and a\n",
        "    divisive position `d`: `x = (1-α)c + αd`. The parameter `α` represents\n",
        "    the salience of the divisive issue.\n",
        "\n",
        "    The simulation demonstrates that as `α` increases, the overall ideological\n",
        "    polarization P(F, x*) of the electorate also increases.\n",
        "\n",
        "    Args:\n",
        "        salience_alphas (List[float]): A list of salience parameters (α) to test.\n",
        "        common_value_dist (Dict[str, Any]): Parameters for the common-value\n",
        "            distribution. E.g., {'low': 4.8, 'high': 5.2} for Uniform.\n",
        "        divisive_issue_dist (Dict[str, Any]): Parameters for the divisive\n",
        "            issue distribution. E.g., {'low': 0, 'high': 10} for Uniform.\n",
        "        polarization_params (Dict[str, Any]): Parameters needed for the\n",
        "            P(F, x*) calculation, including 'x_star' and 'boundaries'.\n",
        "        n_voters (int, optional): The number of voters in the simulation.\n",
        "        random_seed (Optional[int], optional): Seed for the random number\n",
        "            generator to ensure reproducibility.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame indexed by 'alpha' showing the resulting\n",
        "                      'polarization_value' for each salience level.\n",
        "    \"\"\"\n",
        "    # --- Step 1: Setup Simulation Framework ---\n",
        "    # Set the random seed for reproducibility.\n",
        "    rng = np.random.default_rng(random_seed)\n",
        "\n",
        "    # Generate the underlying positions for all voters once.\n",
        "    # c ~ Uniform(low, high)\n",
        "    c_positions = rng.uniform(\n",
        "        low=common_value_dist['low'],\n",
        "        high=common_value_dist['high'],\n",
        "        size=n_voters\n",
        "    )\n",
        "    # d ~ Uniform(low, high)\n",
        "    d_positions = rng.uniform(\n",
        "        low=divisive_issue_dist['low'],\n",
        "        high=divisive_issue_dist['high'],\n",
        "        size=n_voters\n",
        "    )\n",
        "\n",
        "    results = []\n",
        "    # Iterate through each specified salience level alpha.\n",
        "    for alpha in salience_alphas:\n",
        "        # --- Step 2: Generate Ideological Positions ---\n",
        "        # Calculate the final ideological positions based on the model.\n",
        "        # x = (1-α)c + αd\n",
        "        x_positions = (1 - alpha) * c_positions + alpha * d_positions\n",
        "\n",
        "        # --- Step 3: Calculate Polarization of the Simulated Electorate ---\n",
        "        # To calculate P(F, x*), we first need the empirical CDF of x.\n",
        "        # For a simulation, all weights are equal (1/n_voters).\n",
        "        # First, create the PMF by getting unique values and their counts.\n",
        "        unique_pos, counts = np.unique(x_positions, return_counts=True)\n",
        "        pmf_df = pd.DataFrame({'position': unique_pos, 'pmf_value': counts / n_voters})\n",
        "\n",
        "        # Then, create the CDF from the PMF.\n",
        "        pmf_df['cdf_value'] = pmf_df['pmf_value'].cumsum()\n",
        "        cdf_df = pmf_df[['position', 'cdf_value']].set_index('position')\n",
        "\n",
        "        # Now, calculate the polarization index using the function from Task 3.\n",
        "        polarization_value = calculate_polarization_index(\n",
        "            cdf_df=cdf_df,\n",
        "            x_star=polarization_params['x_star'],\n",
        "            boundaries=polarization_params['boundaries']\n",
        "        )\n",
        "\n",
        "        results.append({'alpha': alpha, 'polarization_value': polarization_value})\n",
        "\n",
        "    # --- Step 4: Return Structured Results ---\n",
        "    # Convert the list of results into a DataFrame, indexed by alpha.\n",
        "    results_df = pd.DataFrame(results).set_index('alpha')\n",
        "    return results_df\n"
      ],
      "metadata": {
        "id": "VeS_yTATKgwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 12: Robustness Checks\n",
        "\n",
        "def run_polarization_pipeline(\n",
        "    anes_df: pd.DataFrame,\n",
        "    params: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes the complete end-to-end polarization research pipeline.\n",
        "\n",
        "    This orchestrator function serves as the main entry point for the entire\n",
        "    analysis. It takes raw ANES data and a comprehensive parameter dictionary,\n",
        "    and executes the full sequence of validated, rigorous analytical steps:\n",
        "    1.  Parameter Validation\n",
        "    2.  Data Cleansing\n",
        "    3.  Data Preprocessing (CDF Generation)\n",
        "    4.  Polarization Index Computation\n",
        "    5.  Temporal, Election, and Cleavage Point Analysis\n",
        "    6.  Traditional Measure Calculation and Comparative Analysis\n",
        "    7.  Theoretical Model Extensions\n",
        "\n",
        "    Args:\n",
        "        anes_df (pd.DataFrame): The raw ANES survey data.\n",
        "        params (Dict[str, Any]): A dictionary containing all necessary parameters\n",
        "            for the analysis, including 'central_points_params',\n",
        "            'boundaries_params', 'cleavage_finder_params', etc.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A comprehensive dictionary containing the key output\n",
        "                        DataFrames from each major stage of the analysis.\n",
        "    \"\"\"\n",
        "    # --- Task 0: Parameter Validation ---\n",
        "    # First, validate all inputs to ensure the pipeline can run successfully.\n",
        "    print(\"Step 0: Validating parameters...\")\n",
        "    validate_parameters(\n",
        "        anes_df=anes_df,\n",
        "        central_points_params=params['central_points_params'],\n",
        "        boundaries_params=params['boundaries_params'],\n",
        "        integration_params=params['integration_params'],\n",
        "        cleavage_finder_params=params['cleavage_finder_params']\n",
        "    )\n",
        "    print(\"...Parameters validated successfully.\")\n",
        "\n",
        "    # --- Task 1: Data Cleansing ---\n",
        "    print(\"\\nStep 1: Cleansing raw ANES data...\")\n",
        "    cleaned_df = clean_anes_data(\n",
        "        anes_df=anes_df,\n",
        "        missing_value_map=params.get('missing_value_map') # Use default if not provided\n",
        "    )\n",
        "    print(\"...Data cleansing complete.\")\n",
        "\n",
        "    # --- Task 2: Data Preprocessing ---\n",
        "    print(\"\\nStep 2: Preprocessing data and generating CDFs...\")\n",
        "    preprocessed_data = preprocess_for_polarization(cleaned_df=cleaned_df)\n",
        "    print(\"...Preprocessing complete.\")\n",
        "\n",
        "    # --- Task 4: Compute Polarization Measures ---\n",
        "    # Task 3 is the core calculation function, which is called by Task 4.\n",
        "    print(\"\\nStep 4: Computing flexible polarization measures P(F, x*)...\")\n",
        "    polarization_results = compute_all_polarization_measures(\n",
        "        preprocessed_data=preprocessed_data,\n",
        "        central_points_params=params['central_points_params'],\n",
        "        boundaries_params=params['boundaries_params']\n",
        "    )\n",
        "    print(\"...P(F, x*) computation complete.\")\n",
        "\n",
        "    # --- Task 5: Temporal Analysis ---\n",
        "    print(\"\\nStep 5: Preparing data for temporal analysis...\")\n",
        "    temporal_analysis_data = prepare_temporal_analysis(\n",
        "        polarization_results=polarization_results,\n",
        "        midpoints=params['midpoints']\n",
        "    )\n",
        "    print(\"...Temporal analysis data prepared.\")\n",
        "\n",
        "    # --- Task 6: Election Year Analysis ---\n",
        "    print(\"\\nStep 6: Analyzing election year effects...\")\n",
        "    election_year_analysis = analyze_election_year_effects(\n",
        "        polarization_results=polarization_results,\n",
        "        election_years=params['election_years_for_analysis']\n",
        "    )\n",
        "    print(\"...Election year analysis complete.\")\n",
        "\n",
        "    # --- Task 8: Identify Cleavage Points ---\n",
        "    # Task 7 is the core algorithm, which is called by Task 8.\n",
        "    print(\"\\nStep 8: Identifying and analyzing cleavage points...\")\n",
        "    cleavage_analysis = identify_and_analyze_cleavage_points(\n",
        "        polarization_results=polarization_results,\n",
        "        cleavage_finder_params=params['cleavage_finder_params'],\n",
        "        midpoints=params['midpoints']\n",
        "    )\n",
        "    print(\"...Cleavage point analysis complete.\")\n",
        "\n",
        "    # --- Task 9: Compute Traditional Polarization Measures ---\n",
        "    print(\"\\nStep 9: Computing traditional polarization measures...\")\n",
        "    traditional_measures = compute_traditional_measures(\n",
        "        cleaned_df=cleaned_df,\n",
        "        centrist_definitions=params['centrist_definitions']\n",
        "    )\n",
        "    print(\"...Traditional measures computed.\")\n",
        "\n",
        "    # --- Task 10: Comparative Analysis ---\n",
        "    print(\"\\nStep 10: Creating comparative analysis framework...\")\n",
        "    comparative_framework = create_comparative_analysis_framework(\n",
        "        polarization_results=polarization_results,\n",
        "        traditional_measures=traditional_measures\n",
        "    )\n",
        "    print(\"...Comparative framework created.\")\n",
        "\n",
        "    # --- Task 11: Theoretical Extensions ---\n",
        "    # Note: These are illustrative and not dependent on the main pipeline's flow.\n",
        "    # They are included here to demonstrate their use.\n",
        "    print(\"\\nStep 11: Running theoretical extension models...\")\n",
        "    salience_simulation_results = simulate_issue_salience_effect(\n",
        "        **params['salience_simulation_params']\n",
        "    )\n",
        "    print(\"...Theoretical extensions complete.\")\n",
        "\n",
        "    # --- Final Output Assembly ---\n",
        "    # Compile all key results into a single output dictionary.\n",
        "    final_results = {\n",
        "        \"cleaned_data\": cleaned_df,\n",
        "        \"polarization_results\": polarization_results,\n",
        "        \"temporal_analysis_data\": temporal_analysis_data,\n",
        "        \"election_year_analysis\": election_year_analysis,\n",
        "        \"cleavage_analysis\": cleavage_analysis,\n",
        "        \"traditional_measures\": traditional_measures,\n",
        "        \"comparative_framework\": comparative_framework,\n",
        "        \"salience_simulation_results\": salience_simulation_results\n",
        "    }\n",
        "    print(\"\\n--- Polarization Pipeline Finished ---\")\n",
        "    return final_results\n",
        "\n",
        "def run_robustness_analysis(\n",
        "    anes_df: pd.DataFrame,\n",
        "    params: Dict[str, Any]\n",
        ") -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Performs a robustness analysis by running the full pipeline under\n",
        "    different methodological assumptions.\n",
        "\n",
        "    A critical step in any rigorous quantitative analysis is to assess the\n",
        "    sensitivity of the findings to key methodological choices. This function\n",
        "    facilitates this by running the entire `run_polarization_pipeline` under\n",
        "    a set of predefined scenarios.\n",
        "\n",
        "    The primary check implemented here is the sensitivity to survey weights:\n",
        "    1.  **'Weighted' Scenario:** The baseline analysis using the provided ANES\n",
        "        survey weights.\n",
        "    2.  **'Unweighted' Scenario:** The analysis is re-run assuming every\n",
        "        respondent has an equal weight of 1.0.\n",
        "\n",
        "    By comparing the outputs of these scenarios (e.g., if the identified\n",
        "    cleavage points remain the same), one can assess the robustness of the\n",
        "    conclusions.\n",
        "\n",
        "    Args:\n",
        "        anes_df (pd.DataFrame): The raw ANES survey data.\n",
        "        params (Dict[str, Any]): The comprehensive parameter dictionary for the\n",
        "                                 analysis pipeline.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Dict[str, Any]]:\n",
        "            A dictionary where keys are the scenario names ('weighted',\n",
        "            'unweighted') and values are the complete results dictionaries\n",
        "            produced by the pipeline for that scenario.\n",
        "    \"\"\"\n",
        "    # --- Define Robustness Scenarios ---\n",
        "    # Each scenario is a dictionary defining its name and a function to modify\n",
        "    # the input DataFrame according to the scenario's assumption.\n",
        "    scenarios = [\n",
        "        {\n",
        "            \"name\": \"weighted\",\n",
        "            \"modifier\": lambda df: df.copy() # Baseline: use the data as is.\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"unweighted\",\n",
        "            \"modifier\": lambda df: df.assign(weight=1.0) # Set all weights to 1.0.\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Initialize a dictionary to store the results from all scenarios.\n",
        "    robustness_results = {}\n",
        "\n",
        "    # --- Iterate Through Scenarios and Run Pipeline ---\n",
        "    print(\"--- Starting Robustness Analysis ---\")\n",
        "    for scenario in scenarios:\n",
        "        scenario_name = scenario['name']\n",
        "        modifier_func = scenario['modifier']\n",
        "\n",
        "        print(f\"\\n--- Running Scenario: {scenario_name.upper()} ---\")\n",
        "\n",
        "        # Apply the scenario's modification to the input DataFrame.\n",
        "        scenario_df = modifier_func(anes_df)\n",
        "\n",
        "        # Run the entire end-to-end pipeline with the modified data.\n",
        "        # A try-except block makes the overall analysis resilient to a failure\n",
        "        # in a single scenario.\n",
        "        try:\n",
        "            scenario_result = run_polarization_pipeline(\n",
        "                anes_df=scenario_df,\n",
        "                params=params\n",
        "            )\n",
        "            # Store the complete results dictionary under the scenario's name.\n",
        "            robustness_results[scenario_name] = scenario_result\n",
        "        except Exception as e:\n",
        "            # If a pipeline run fails, report the error and continue.\n",
        "            print(f\"!!! SCENARIO '{scenario_name}' FAILED: {e} !!!\")\n",
        "            robustness_results[scenario_name] = {\"error\": str(e)}\n",
        "\n",
        "    print(\"\\n--- Robustness Analysis Finished ---\")\n",
        "    # Return the dictionary containing the results from all scenarios.\n",
        "    return robustness_results\n"
      ],
      "metadata": {
        "id": "7PanVzi5LdnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 13: Visualization and Reporting\n",
        "\n",
        "def _plot_temporal_trends(\n",
        "    left_panel_df: pd.DataFrame,\n",
        "    right_panel_df: pd.DataFrame,\n",
        "    scale_name: str\n",
        ") -> plt.Figure:\n",
        "    \"\"\"Helper function to generate a two-panel temporal trend plot for one scale.\"\"\"\n",
        "    # Set a professional plot style.\n",
        "    sns.set_theme(style=\"whitegrid\")\n",
        "    # Create a figure with two subplots, side-by-side.\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6), sharey=True)\n",
        "    fig.suptitle(f\"Temporal Evolution of Polarization: {scale_name.replace('_', ' ').title()}\", fontsize=16)\n",
        "\n",
        "    # --- Left Panel ---\n",
        "    # Plot the time series for x* values at or below the midpoint.\n",
        "    sns.lineplot(\n",
        "        data=left_panel_df,\n",
        "        x='year_numeric',\n",
        "        y='polarization_value',\n",
        "        hue='x_star',\n",
        "        style='x_star',\n",
        "        markers=True,\n",
        "        palette='viridis',\n",
        "        ax=axes[0]\n",
        "    )\n",
        "    axes[0].set_title('Left-of-Center / Centrist Positions')\n",
        "    axes[0].set_xlabel('Year')\n",
        "    axes[0].set_ylabel(r'Polarization Index $P(F, x^*)$')\n",
        "    axes[0].legend(title=r'$x^*$')\n",
        "\n",
        "    # --- Right Panel ---\n",
        "    # Plot the time series for x* values above the midpoint.\n",
        "    sns.lineplot(\n",
        "        data=right_panel_df,\n",
        "        x='year_numeric',\n",
        "        y='polarization_value',\n",
        "        hue='x_star',\n",
        "        style='x_star',\n",
        "        markers=True,\n",
        "        palette='plasma',\n",
        "        ax=axes[1]\n",
        "    )\n",
        "    axes[1].set_title('Right-of-Center Positions')\n",
        "    axes[1].set_xlabel('Year')\n",
        "    axes[1].set_ylabel('') # Shared Y-axis, so no label needed.\n",
        "    axes[1].legend(title=r'$x^*$')\n",
        "\n",
        "    # Improve layout and return the figure object.\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "    return fig\n",
        "\n",
        "def _plot_election_effects(\n",
        "    election_df: pd.DataFrame,\n",
        "    election_year: int\n",
        ") -> plt.Figure:\n",
        "    \"\"\"Helper function to generate a pre- vs. post-election plot.\"\"\"\n",
        "    sns.set_theme(style=\"whitegrid\")\n",
        "    # Get the unique scales present in this election year's data.\n",
        "    scales = election_df.index.get_level_values('scale').unique()\n",
        "    # Create a figure with one subplot for each scale.\n",
        "    fig, axes = plt.subplots(1, len(scales), figsize=(8 * len(scales), 6), sharey=True)\n",
        "    if len(scales) == 1: axes = [axes] # Ensure axes is always iterable\n",
        "    fig.suptitle(f\"Pre- vs. Post-Election Polarization: {election_year}\", fontsize=16)\n",
        "\n",
        "    for i, scale in enumerate(scales):\n",
        "        # Select data for the current scale.\n",
        "        scale_data = election_df.xs(scale, level='scale')\n",
        "        # Plot pre-election values.\n",
        "        axes[i].plot(\n",
        "            scale_data.index, scale_data['polarization_pre'],\n",
        "            marker='o', linestyle='--', label='Pre-Election', alpha=0.8\n",
        "        )\n",
        "        # Plot post-election values.\n",
        "        axes[i].plot(\n",
        "            scale_data.index, scale_data['polarization_post'],\n",
        "            marker='x', linestyle='-', label='Post-Election', alpha=0.8\n",
        "        )\n",
        "        axes[i].set_title(f\"Scale: {scale.replace('_', ' ').title()}\")\n",
        "        axes[i].set_xlabel(r'Central Point $x^*$')\n",
        "        axes[i].legend()\n",
        "\n",
        "    # Set the Y-label only for the first subplot.\n",
        "    axes[0].set_ylabel(r'Polarization Index $P(F, x^*)$')\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "    return fig\n",
        "\n",
        "def generate_report_visuals(\n",
        "    pipeline_results: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Generates a report of key visuals and tables from the pipeline results.\n",
        "\n",
        "    This function takes the comprehensive results from the pipeline orchestrator\n",
        "    and produces a set of publication-quality outputs, including:\n",
        "    1.  Temporal Trend Plots: Multi-panel plots showing the evolution of\n",
        "        P(F, x*) over time, replicating the style of Figures 1 & 2.\n",
        "    2.  Election Year Plots: Before-and-after plots showing the change in\n",
        "        polarization around key elections, replicating the style of Figure 3.\n",
        "    3.  Summary Tables: Formatted versions of the key analytical DataFrames\n",
        "        (cleavage points, comparative analysis) suitable for inclusion in a\n",
        "        report (e.g., in LaTeX or HTML format).\n",
        "\n",
        "    Args:\n",
        "        pipeline_results (Dict[str, Any]):\n",
        "            The dictionary of results produced by `run_polarization_pipeline`.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]:\n",
        "            A dictionary containing the generated matplotlib Figure objects and\n",
        "            formatted table strings, keyed by descriptive names.\n",
        "    \"\"\"\n",
        "    # Initialize the dictionary to hold all report assets.\n",
        "    report = {\"plots\": {}, \"tables\": {}}\n",
        "\n",
        "    # --- Step 1: Create Publication-Quality Temporal Trend Plots ---\n",
        "    print(\"Generating temporal trend plots...\")\n",
        "    temporal_data = pipeline_results['temporal_analysis_data']\n",
        "    for scale, panel_data in temporal_data.items():\n",
        "        fig = _plot_temporal_trends(\n",
        "            left_panel_df=panel_data['left_panel']['long'],\n",
        "            right_panel_df=panel_data['right_panel']['long'],\n",
        "            scale_name=scale\n",
        "        )\n",
        "        report['plots'][f'temporal_trends_{scale}'] = fig\n",
        "        plt.close(fig) # Close plot to prevent it from displaying prematurely.\n",
        "\n",
        "    # --- Step 2: Generate Election Year Comparison Visualizations ---\n",
        "    print(\"Generating election year effect plots...\")\n",
        "    election_data = pipeline_results['election_year_analysis']\n",
        "    if not election_data.empty:\n",
        "        for year in election_data.index.get_level_values('election_year').unique():\n",
        "            year_df = election_data.xs(year, level='election_year')\n",
        "            fig = _plot_election_effects(year_df, year)\n",
        "            report['plots'][f'election_effects_{year}'] = fig\n",
        "            plt.close(fig)\n",
        "\n",
        "    # --- Step 3: Compile Comprehensive Results Tables ---\n",
        "    print(\"Generating summary tables...\")\n",
        "    # Format the cleavage analysis summary table as a LaTeX string.\n",
        "    cleavage_df = pipeline_results['cleavage_analysis']\n",
        "    if not cleavage_df.empty:\n",
        "        report['tables']['cleavage_summary_latex'] = cleavage_df.to_latex(\n",
        "            caption=\"Summary of Identified Cleavage Points\",\n",
        "            label=\"tab:cleavage\",\n",
        "            float_format=\"%.2f\"\n",
        "        )\n",
        "\n",
        "    # Format the correlation summary table as a LaTeX string.\n",
        "    corr_summary_df = pipeline_results['comparative_framework']['correlation_summary']\n",
        "    if not corr_summary_df.empty:\n",
        "        report['tables']['correlation_summary_latex'] = corr_summary_df.to_latex(\n",
        "            caption=\"Correlation Between Traditional and Flexible Polarization Measures\",\n",
        "            label=\"tab:correlation\",\n",
        "            float_format=\"%.3f\"\n",
        "        )\n",
        "\n",
        "    print(\"\\n--- Report Generation Finished ---\")\n",
        "    return report\n"
      ],
      "metadata": {
        "id": "DLAFl1wQPAsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Master Orchestrator\n",
        "\n",
        "def execute_full_research_project(\n",
        "    anes_df: pd.DataFrame,\n",
        "    params: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes the complete, end-to-end polarization research project.\n",
        "\n",
        "    This master orchestrator function serves as the single entry point to run\n",
        "    the entire analysis suite, from raw data to final report assets. It encapsulates\n",
        "    the full research workflow, including the baseline analysis, robustness checks,\n",
        "    and the generation of all tables and visualizations.\n",
        "\n",
        "    The workflow is as follows:\n",
        "    1.  **Main Pipeline Run:** Executes the `run_polarization_pipeline` function\n",
        "        to generate the primary set of findings based on the default parameters\n",
        "        (typically using survey weights).\n",
        "    2.  **Report Generation:** Uses the results from the main pipeline run to\n",
        "        call `generate_report_visuals`, creating all necessary plots and tables\n",
        "        for the primary report.\n",
        "    3.  **Robustness Analysis:** Executes the `run_robustness_analysis` function,\n",
        "        which re-runs the entire pipeline under different methodological\n",
        "        scenarios (e.g., weighted vs. unweighted) to test the sensitivity\n",
        "        of the findings.\n",
        "    4.  **Result Consolidation:** Assembles all outputs into a final, comprehensive,\n",
        "        and hierarchically structured dictionary, which represents the complete\n",
        "        output of the research project.\n",
        "\n",
        "    Args:\n",
        "        anes_df (pd.DataFrame): The raw ANES survey data.\n",
        "        params (Dict[str, Any]): A comprehensive dictionary containing all\n",
        "            parameters required for every stage of the analysis.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]:\n",
        "            A master dictionary containing the complete project results,\n",
        "            structured as follows:\n",
        "            {\n",
        "                'main_analysis': {\n",
        "                    'results': Dict[str, pd.DataFrame], # Output from pipeline\n",
        "                    'report_assets': Dict[str, Any]     # Output from visuals\n",
        "                },\n",
        "                'robustness_analysis': Dict[str, Dict] # Output from robustness\n",
        "            }\n",
        "    \"\"\"\n",
        "    print(\"=============================================\")\n",
        "    print(\"=== EXECUTING FULL POLARIZATION PROJECT ===\")\n",
        "    print(\"=============================================\")\n",
        "\n",
        "    # --- Step 1: Execute the Main Analysis Pipeline ---\n",
        "    # This run constitutes the primary findings of the research.\n",
        "    print(\"\\n>>> STAGE 1: RUNNING MAIN ANALYSIS PIPELINE...\")\n",
        "    main_pipeline_results = run_polarization_pipeline(\n",
        "        anes_df=anes_df,\n",
        "        params=params\n",
        "    )\n",
        "    print(\">>> STAGE 1 COMPLETE.\")\n",
        "\n",
        "    # --- Step 2: Generate Report Visuals for the Main Analysis ---\n",
        "    print(\"\\n>>> STAGE 2: GENERATING REPORT ASSETS FOR MAIN ANALYSIS...\")\n",
        "    report_assets = generate_report_visuals(\n",
        "        pipeline_results=main_pipeline_results\n",
        "    )\n",
        "    print(\">>> STAGE 2 COMPLETE.\")\n",
        "\n",
        "    # --- Step 3: Execute the Robustness Analysis ---\n",
        "    # This stage re-runs the pipeline under different scenarios to test sensitivity.\n",
        "    print(\"\\n>>> STAGE 3: RUNNING ROBUSTNESS ANALYSIS...\")\n",
        "    robustness_analysis_results = run_robustness_analysis(\n",
        "        anes_df=anes_df,\n",
        "        params=params\n",
        "    )\n",
        "    print(\">>> STAGE 3 COMPLETE.\")\n",
        "\n",
        "    # --- Step 4: Assemble the Final Structured Output Dictionary ---\n",
        "    # This is the final deliverable, conforming to the specification of Task 14.\n",
        "    # It contains all analytical products in a clean, hierarchical structure.\n",
        "    master_results_dictionary = {\n",
        "        'main_analysis': {\n",
        "            'results': main_pipeline_results,\n",
        "            'report_assets': report_assets\n",
        "        },\n",
        "        'robustness_analysis': robustness_analysis_results\n",
        "    }\n",
        "\n",
        "    print(\"\\n=============================================\")\n",
        "    print(\"=== PROJECT EXECUTION SUCCESSFULLY COMPLETED ===\")\n",
        "    print(\"=============================================\")\n",
        "\n",
        "    return master_results_dictionary\n"
      ],
      "metadata": {
        "id": "3QmSkgmjTYsB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}